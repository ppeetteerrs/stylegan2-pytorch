{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"StyleGAN2 Pytorch - Typed, Commented, Installable :) A simple, typed, commented Pytorch implementation of StyleGAN2. This implementation is adapted from here . This implementation seems more stable and editable than the over-engineered official implementation. The focus of this repository is simplicity and readability. If there are any bugs / issues, please kindly let me know or submit a pull request! Refer to my blog post for an explanation on the custom CUDA kernels. The profiling code to optimize the custom operations is here . Installation pip install stylegan2-torch Training Tips Use a multi-GPU setup. An RTX 3090 can handle batch size of up to 8 at 1024 resolution. Based on experience, batch size of 8 works but 16 or 32 should be safer. Use LMDB dataset + SSD storage + multiple dataloader workers (and a big enough prefetch factor to cache at least one batch ahead). You never know how much time you waste on dataloading until you optimize it. For me, that shorted the training time by 30% (more time-saving than the custom CUDA kernels). Known Issues Pytorch is known to cause random reboots when using non-deterministic algorithms. Set torch.use_deterministic_algorithms(True) if you encounter that. To Dos / Won't Dos Tidy up conv2d_gradfix.py and fused_act.py . These were just copied over from the original repo so they are still ugly and untidy. Provide pretrained model conversion method (not that hard tbh, just go map the state_dict keys). Clean up util functions to aid training loop design.","title":"Home"},{"location":"#stylegan2-pytorch-typed-commented-installable","text":"A simple, typed, commented Pytorch implementation of StyleGAN2. This implementation is adapted from here . This implementation seems more stable and editable than the over-engineered official implementation. The focus of this repository is simplicity and readability. If there are any bugs / issues, please kindly let me know or submit a pull request! Refer to my blog post for an explanation on the custom CUDA kernels. The profiling code to optimize the custom operations is here .","title":"StyleGAN2 Pytorch - Typed, Commented, Installable :)"},{"location":"#installation","text":"pip install stylegan2-torch","title":"Installation"},{"location":"#training-tips","text":"Use a multi-GPU setup. An RTX 3090 can handle batch size of up to 8 at 1024 resolution. Based on experience, batch size of 8 works but 16 or 32 should be safer. Use LMDB dataset + SSD storage + multiple dataloader workers (and a big enough prefetch factor to cache at least one batch ahead). You never know how much time you waste on dataloading until you optimize it. For me, that shorted the training time by 30% (more time-saving than the custom CUDA kernels).","title":"Training Tips"},{"location":"#known-issues","text":"Pytorch is known to cause random reboots when using non-deterministic algorithms. Set torch.use_deterministic_algorithms(True) if you encounter that.","title":"Known Issues"},{"location":"#to-dos-wont-dos","text":"Tidy up conv2d_gradfix.py and fused_act.py . These were just copied over from the original repo so they are still ugly and untidy. Provide pretrained model conversion method (not that hard tbh, just go map the state_dict keys). Clean up util functions to aid training loop design.","title":"To Dos / Won't Dos"},{"location":"all/","text":"Resolution module-attribute Resolution = Literal [ 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 , 8192 ] __all__ module-attribute __all__ = [ \"Discriminator\" , \"Generator\" , \"Resolution\" , \"default_channels\" , \"Blur\" , \"EqualConv2d\" , \"EqualLeakyReLU\" , \"EqualLinear\" , ] __author__ module-attribute __author__ = 'Peter Yuen' __email__ module-attribute __email__ = 'ppeetteerrsx@gmail.com' __version__ module-attribute __version__ = '0.0.0' default_channels module-attribute default_channels : Dict [ Resolution , int ] = { 4 : 512 , 8 : 512 , 16 : 512 , 32 : 512 , 64 : 512 , 128 : 256 , 256 : 128 , 512 : 64 , 1024 : 32 , } Blur Blur ( blur_kernel : List [ int ], factor : int , kernel_size : int ) Bases: nn . Module Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Parameters: Name Type Description Default input Tensor (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H * U, W * U) Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Parameters: Name Type Description Default input Tensor (N, C, H, W) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) Source code in stylegan2_torch/equalized_lr.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , blur_kernel : List [ int ], factor : int , kernel_size : int ): \"\"\" Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Case 1: Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Args: input (Tensor): (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H * U, W * U) Case 2: Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Args: input (Tensor): (N, C, H, W) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) \"\"\" super () . __init__ () if factor > 0 : p = ( len ( blur_kernel ) - factor ) - ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 + factor - 1 pad1 = p // 2 + 1 else : p = ( len ( blur_kernel ) - abs ( factor )) + ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 pad1 = p // 2 # Factor to compensate for averaging with zeros if upsampling self . kernel : Tensor self . register_buffer ( \"kernel\" , make_kernel ( blur_kernel , factor if factor > 0 else 1 ) ) self . pad = ( pad0 , pad1 ) __call__ class-attribute __call__ = proxy ( forward ) kernel instance-attribute kernel : Tensor = None pad instance-attribute pad = ( pad0 , pad1 ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 171 172 def forward ( self , input : Tensor ) -> Tensor : return upfirdn2d ( input , self . kernel , pad = self . pad ) Discriminator Discriminator ( resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Discriminator module Source code in stylegan2_torch/discriminator/__init__.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () # FromRGB followed by ResBlock self . n_layers = int ( math . log ( resolution , 2 )) self . blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), * [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** ( i - 1 )], blur_kernel ) for i in range ( self . n_layers , 2 , - 1 ) ], ) # Minibatch std settings self . stddev_group = 4 self . stddev_feat = 1 # Final layers self . final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 ) self . final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ]) self . final_linear = EqualLinear ( channels [ 4 ], 1 ) __call__ class-attribute __call__ = proxy ( forward ) blocks instance-attribute blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** i - 1 ], blur_kernel , ) for i in range ( self . n_layers , 2 , - 1 ) ], ) final_conv instance-attribute final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 ) final_linear instance-attribute final_linear = EqualLinear ( channels [ 4 ], 1 ) final_relu instance-attribute final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ] ) n_layers instance-attribute n_layers = int ( math . log ( resolution , 2 )) stddev_feat instance-attribute stddev_feat = 1 stddev_group instance-attribute stddev_group = 4 forward forward ( input : Tensor , * , return_features : bool = False ) Source code in stylegan2_torch/discriminator/__init__.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def forward ( self , input : Tensor , * , return_features : bool = False ): # Downsampling blocks out : Tensor = self . blocks ( input ) # Minibatch stddev layer in Progressive GAN https://www.youtube.com/watch?v=V1qQXb9KcDY # Purpose is to provide variational information to the discriminator to prevent mode collapse # Other layers do not cross sample boundaries batch , channel , height , width = out . shape n_groups = min ( batch , self . stddev_group ) stddev = out . view ( n_groups , - 1 , self . stddev_feat , channel // self . stddev_feat , height , width ) stddev = torch . sqrt ( stddev . var ( 0 , unbiased = False ) + 1e-8 ) stddev = stddev . mean ([ 2 , 3 , 4 ], keepdim = True ) . squeeze ( 2 ) stddev = stddev . repeat ( n_groups , 1 , height , width ) out = torch . cat ([ out , stddev ], 1 ) # Final layers out = self . final_conv ( out ) features = self . final_relu ( out . view ( batch , - 1 )) out = self . final_linear ( features ) if return_features : return out , features else : return out EqualConv2d EqualConv2d ( in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ) Bases: nn . Module Conv2d with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) ) # std = gain / sqrt(fan_in) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) self . stride = stride self . padding = padding self . bias = Parameter ( torch . zeros ( out_channel )) if bias else None __call__ class-attribute __call__ = proxy ( forward ) bias instance-attribute bias = Parameter ( torch . zeros ( out_channel )) if bias else None padding instance-attribute padding = padding scale instance-attribute scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) stride instance-attribute stride = stride weight instance-attribute weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) ) __repr__ __repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 51 52 53 54 55 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } ,\" f \" { self . weight . shape [ 2 ] } , stride= { self . stride } , padding= { self . padding } )\" ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 42 43 44 45 46 47 48 49 def forward ( self , input : Tensor ) -> Tensor : return conv2d ( input = input , weight = self . weight * self . scale , bias = self . bias , stride = self . stride , padding = self . padding , ) EqualLeakyReLU EqualLeakyReLU ( in_dim : int , out_dim : int , lr_mult : float = 1 ) Bases: nn . Module Leaky ReLU with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , in_dim : int , out_dim : int , lr_mult : float = 1 ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult __call__ class-attribute __call__ = proxy ( forward ) bias instance-attribute bias = Parameter ( torch . zeros ( out_dim )) lr_mult instance-attribute lr_mult = lr_mult scale instance-attribute scale = 1 / math . sqrt ( in_dim ) * lr_mult weight instance-attribute weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) ) __repr__ __repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 115 116 117 118 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 111 112 113 def forward ( self , input : Tensor ) -> Tensor : out = F . linear ( input , self . weight * self . scale ) return fused_leaky_relu ( out , self . bias * self . lr_mult ) EqualLinear EqualLinear ( in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ) Bases: nn . Module Linear with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult __call__ class-attribute __call__ = proxy ( forward ) bias instance-attribute bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init )) lr_mult instance-attribute lr_mult = lr_mult scale instance-attribute scale = 1 / math . sqrt ( in_dim ) * lr_mult weight instance-attribute weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) ) __repr__ __repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 86 87 88 89 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 83 84 def forward ( self , input : Tensor ) -> Tensor : return F . linear ( input , self . weight * self . scale , bias = self . bias * self . lr_mult ) Generator Generator ( resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Generator module Source code in stylegan2_torch/generator/__init__.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () self . latent_dim = latent_dim # Create mapping network self . mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult ) # Create constant input self . input = ConstantInput ( channels [ 4 ], 4 ) # Create Conv, UpConv and ToRGB Blocks self . convs = nn . ModuleList () self . up_convs = nn . ModuleList () self . to_rgbs = nn . ModuleList () self . n_layers = int ( math . log ( resolution , 2 )) self . n_w_plus = self . n_layers * 2 - 2 for layer_idx in range ( 2 , self . n_layers + 1 ): # Upsample condition upsample = layer_idx > 2 # Calculate image size and channels at the layer prev_layer_size = 2 ** ( layer_idx - 1 ) layer_size : Resolution = 2 ** layer_idx layer_channel = channels [ layer_size ] # Upsampling Conv Block if upsample : self . up_convs . append ( UpModConvBlock ( channels [ prev_layer_size ], layer_channel , 3 , latent_dim , 2 , blur_kernel , ) ) # Normal Conv Block self . convs . append ( ModConvBlock ( layer_channel , layer_channel , 3 , latent_dim )) # ToRGB Block self . to_rgbs . append ( ToRGB ( layer_channel , latent_dim , 2 if upsample else 1 , blur_kernel , ) ) __call__ class-attribute __call__ = proxy ( forward ) convs instance-attribute convs = nn . ModuleList () input instance-attribute input = ConstantInput ( channels [ 4 ], 4 ) latent_dim instance-attribute latent_dim = latent_dim mapping instance-attribute mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult ) n_layers instance-attribute n_layers = int ( math . log ( resolution , 2 )) n_w_plus instance-attribute n_w_plus = self . n_layers * 2 - 2 to_rgbs instance-attribute to_rgbs = nn . ModuleList () up_convs instance-attribute up_convs = nn . ModuleList () forward forward ( input : List [ Tensor ], * , return_latents : bool = False , input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , trunc_option : Optional [ Tuple [ float , Tensor ]] = None , mix_index : Optional [ int ] = None , noises : Optional [ List [ Optional [ Tensor ]]] = None ) Source code in stylegan2_torch/generator/__init__.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def forward ( self , # Input tensors (N, latent_dim) input : List [ Tensor ], * , # Return latents return_latents : bool = False , # Type of input tensor input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , # Truncation options trunc_option : Optional [ Tuple [ float , Tensor ]] = None , # Mixing regularization options mix_index : Optional [ int ] = None , # Noise vectors noises : Optional [ List [ Optional [ Tensor ]]] = None , ): # Get w vectors (can have 2 w vectors for mixing regularization) ws : List [ Tensor ] if input_type == \"z\" : ws = [ self . mapping ( z ) for z in input ] else : ws = input # Perform truncation if trunc_option : trunc_coeff , trunc_tensor = trunc_option ws = [ trunc_tensor + trunc_coeff * ( w - trunc_tensor ) for w in ws ] # Mixing regularization (why add dimension 1 not 0 lol) w_plus : Tensor if len ( ws ) == 1 : # No mixing regularization mix_index = self . n_w_plus if input_type == \"w_plus\" : w_plus = ws [ 0 ] else : w_plus = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) else : mix_index = mix_index if mix_index else random . randint ( 1 , self . n_w_plus - 1 ) w_plus1 = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) w_plus2 = ws [ 1 ] . unsqueeze ( 1 ) . repeat ( 1 , self . n_w_plus - mix_index , 1 ) w_plus = torch . cat ([ w_plus1 , w_plus2 ], 1 ) # Get noise noises_ : List [ Optional [ Tensor ]] = ( noises if noises else [ None ] * ( self . n_w_plus - 1 ) ) # Constant input out = self . input ( w_plus ) # References for this weird indexing: # https://github.com/NVlabs/stylegan2-ada-pytorch/issues/50 # https://github.com/rosinality/stylegan2-pytorch/issues/278 img = None for i in range ( self . n_layers - 1 ): if i > 0 : out = self . up_convs [ i - 1 ]( out , w_plus [:, i * 2 - 1 ], noises_ [ i * 2 - 1 ] ) out = self . convs [ i ]( out , w_plus [:, i * 2 ], noises_ [ i * 2 ]) img = self . to_rgbs [ i ]( out , w_plus [:, i * 2 + 1 ], img ) if return_latents : return img , w_plus else : return img mean_latent mean_latent ( n_sample : int , device : str ) -> Tensor Source code in stylegan2_torch/generator/__init__.py 98 99 100 101 102 103 def mean_latent ( self , n_sample : int , device : str ) -> Tensor : mean_latent = self . mapping ( torch . randn ( n_sample , self . latent_dim , device = device ) ) . mean ( 0 , keepdim = True ) mean_latent . detach_ () return mean_latent __docs __docs () Build gh-pages documentation branch. Source code in stylegan2_torch/__init__.py 48 49 50 51 52 53 54 55 def __docs (): # pragma: no cover \"\"\" Build gh-pages documentation branch. \"\"\" shell ( \"cp README.md docs/index.md && \\ mkdocs gh-deploy --force\" ) __serve __serve () Serve local documentation. Source code in stylegan2_torch/__init__.py 37 38 39 40 41 42 43 44 45 def __serve (): # pragma: no cover \"\"\" Serve local documentation. \"\"\" print ( \"serving\" ) shell ( \"cp README.md docs/index.md && \\ mkdocs serve\" ) __test __test () Runs pytest locally and keeps only coverage.xml for GitHub Actions to upload to Codecov. Source code in stylegan2_torch/__init__.py 27 28 29 30 31 32 33 34 def __test (): # pragma: no cover \"\"\" Runs pytest locally and keeps only `coverage.xml` for GitHub Actions to upload to Codecov. \"\"\" shell ( \"pytest --cov=simple_poetry --cov-report xml --cov-report term-missing tests \\ && rm -rf .pytest_cache && rm .coverage\" )","title":"__all__"},{"location":"all/#stylegan2_torch.Resolution","text":"Resolution = Literal [ 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 , 8192 ]","title":"Resolution"},{"location":"all/#stylegan2_torch.__all__","text":"__all__ = [ \"Discriminator\" , \"Generator\" , \"Resolution\" , \"default_channels\" , \"Blur\" , \"EqualConv2d\" , \"EqualLeakyReLU\" , \"EqualLinear\" , ]","title":"__all__"},{"location":"all/#stylegan2_torch.__author__","text":"__author__ = 'Peter Yuen'","title":"__author__"},{"location":"all/#stylegan2_torch.__email__","text":"__email__ = 'ppeetteerrsx@gmail.com'","title":"__email__"},{"location":"all/#stylegan2_torch.__version__","text":"__version__ = '0.0.0'","title":"__version__"},{"location":"all/#stylegan2_torch.default_channels","text":"default_channels : Dict [ Resolution , int ] = { 4 : 512 , 8 : 512 , 16 : 512 , 32 : 512 , 64 : 512 , 128 : 256 , 256 : 128 , 512 : 64 , 1024 : 32 , }","title":"default_channels"},{"location":"all/#stylegan2_torch.Blur","text":"Blur ( blur_kernel : List [ int ], factor : int , kernel_size : int ) Bases: nn . Module Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Parameters: Name Type Description Default input Tensor (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H * U, W * U) Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Parameters: Name Type Description Default input Tensor (N, C, H, W) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) Source code in stylegan2_torch/equalized_lr.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , blur_kernel : List [ int ], factor : int , kernel_size : int ): \"\"\" Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Case 1: Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Args: input (Tensor): (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H * U, W * U) Case 2: Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Args: input (Tensor): (N, C, H, W) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) \"\"\" super () . __init__ () if factor > 0 : p = ( len ( blur_kernel ) - factor ) - ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 + factor - 1 pad1 = p // 2 + 1 else : p = ( len ( blur_kernel ) - abs ( factor )) + ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 pad1 = p // 2 # Factor to compensate for averaging with zeros if upsampling self . kernel : Tensor self . register_buffer ( \"kernel\" , make_kernel ( blur_kernel , factor if factor > 0 else 1 ) ) self . pad = ( pad0 , pad1 )","title":"Blur"},{"location":"all/#stylegan2_torch.equalized_lr.Blur.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"all/#stylegan2_torch.equalized_lr.Blur.kernel","text":"kernel : Tensor = None","title":"kernel"},{"location":"all/#stylegan2_torch.equalized_lr.Blur.pad","text":"pad = ( pad0 , pad1 )","title":"pad"},{"location":"all/#stylegan2_torch.equalized_lr.Blur.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 171 172 def forward ( self , input : Tensor ) -> Tensor : return upfirdn2d ( input , self . kernel , pad = self . pad )","title":"forward()"},{"location":"all/#stylegan2_torch.Discriminator","text":"Discriminator ( resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Discriminator module Source code in stylegan2_torch/discriminator/__init__.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () # FromRGB followed by ResBlock self . n_layers = int ( math . log ( resolution , 2 )) self . blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), * [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** ( i - 1 )], blur_kernel ) for i in range ( self . n_layers , 2 , - 1 ) ], ) # Minibatch std settings self . stddev_group = 4 self . stddev_feat = 1 # Final layers self . final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 ) self . final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ]) self . final_linear = EqualLinear ( channels [ 4 ], 1 )","title":"Discriminator"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.blocks","text":"blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** i - 1 ], blur_kernel , ) for i in range ( self . n_layers , 2 , - 1 ) ], )","title":"blocks"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.final_conv","text":"final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 )","title":"final_conv"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.final_linear","text":"final_linear = EqualLinear ( channels [ 4 ], 1 )","title":"final_linear"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.final_relu","text":"final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ] )","title":"final_relu"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.n_layers","text":"n_layers = int ( math . log ( resolution , 2 ))","title":"n_layers"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.stddev_feat","text":"stddev_feat = 1","title":"stddev_feat"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.stddev_group","text":"stddev_group = 4","title":"stddev_group"},{"location":"all/#stylegan2_torch.discriminator.Discriminator.forward","text":"forward ( input : Tensor , * , return_features : bool = False ) Source code in stylegan2_torch/discriminator/__init__.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def forward ( self , input : Tensor , * , return_features : bool = False ): # Downsampling blocks out : Tensor = self . blocks ( input ) # Minibatch stddev layer in Progressive GAN https://www.youtube.com/watch?v=V1qQXb9KcDY # Purpose is to provide variational information to the discriminator to prevent mode collapse # Other layers do not cross sample boundaries batch , channel , height , width = out . shape n_groups = min ( batch , self . stddev_group ) stddev = out . view ( n_groups , - 1 , self . stddev_feat , channel // self . stddev_feat , height , width ) stddev = torch . sqrt ( stddev . var ( 0 , unbiased = False ) + 1e-8 ) stddev = stddev . mean ([ 2 , 3 , 4 ], keepdim = True ) . squeeze ( 2 ) stddev = stddev . repeat ( n_groups , 1 , height , width ) out = torch . cat ([ out , stddev ], 1 ) # Final layers out = self . final_conv ( out ) features = self . final_relu ( out . view ( batch , - 1 )) out = self . final_linear ( features ) if return_features : return out , features else : return out","title":"forward()"},{"location":"all/#stylegan2_torch.EqualConv2d","text":"EqualConv2d ( in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ) Bases: nn . Module Conv2d with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) ) # std = gain / sqrt(fan_in) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) self . stride = stride self . padding = padding self . bias = Parameter ( torch . zeros ( out_channel )) if bias else None","title":"EqualConv2d"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.bias","text":"bias = Parameter ( torch . zeros ( out_channel )) if bias else None","title":"bias"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.padding","text":"padding = padding","title":"padding"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.scale","text":"scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 )","title":"scale"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.stride","text":"stride = stride","title":"stride"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.weight","text":"weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) )","title":"weight"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.__repr__","text":"__repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 51 52 53 54 55 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } ,\" f \" { self . weight . shape [ 2 ] } , stride= { self . stride } , padding= { self . padding } )\" )","title":"__repr__()"},{"location":"all/#stylegan2_torch.equalized_lr.EqualConv2d.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 42 43 44 45 46 47 48 49 def forward ( self , input : Tensor ) -> Tensor : return conv2d ( input = input , weight = self . weight * self . scale , bias = self . bias , stride = self . stride , padding = self . padding , )","title":"forward()"},{"location":"all/#stylegan2_torch.EqualLeakyReLU","text":"EqualLeakyReLU ( in_dim : int , out_dim : int , lr_mult : float = 1 ) Bases: nn . Module Leaky ReLU with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , in_dim : int , out_dim : int , lr_mult : float = 1 ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult","title":"EqualLeakyReLU"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLeakyReLU.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLeakyReLU.bias","text":"bias = Parameter ( torch . zeros ( out_dim ))","title":"bias"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLeakyReLU.lr_mult","text":"lr_mult = lr_mult","title":"lr_mult"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLeakyReLU.scale","text":"scale = 1 / math . sqrt ( in_dim ) * lr_mult","title":"scale"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLeakyReLU.weight","text":"weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) )","title":"weight"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLeakyReLU.__repr__","text":"__repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 115 116 117 118 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" )","title":"__repr__()"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLeakyReLU.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 111 112 113 def forward ( self , input : Tensor ) -> Tensor : out = F . linear ( input , self . weight * self . scale ) return fused_leaky_relu ( out , self . bias * self . lr_mult )","title":"forward()"},{"location":"all/#stylegan2_torch.EqualLinear","text":"EqualLinear ( in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ) Bases: nn . Module Linear with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult","title":"EqualLinear"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLinear.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLinear.bias","text":"bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init ))","title":"bias"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLinear.lr_mult","text":"lr_mult = lr_mult","title":"lr_mult"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLinear.scale","text":"scale = 1 / math . sqrt ( in_dim ) * lr_mult","title":"scale"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLinear.weight","text":"weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) )","title":"weight"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLinear.__repr__","text":"__repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 86 87 88 89 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" )","title":"__repr__()"},{"location":"all/#stylegan2_torch.equalized_lr.EqualLinear.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 83 84 def forward ( self , input : Tensor ) -> Tensor : return F . linear ( input , self . weight * self . scale , bias = self . bias * self . lr_mult )","title":"forward()"},{"location":"all/#stylegan2_torch.Generator","text":"Generator ( resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Generator module Source code in stylegan2_torch/generator/__init__.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () self . latent_dim = latent_dim # Create mapping network self . mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult ) # Create constant input self . input = ConstantInput ( channels [ 4 ], 4 ) # Create Conv, UpConv and ToRGB Blocks self . convs = nn . ModuleList () self . up_convs = nn . ModuleList () self . to_rgbs = nn . ModuleList () self . n_layers = int ( math . log ( resolution , 2 )) self . n_w_plus = self . n_layers * 2 - 2 for layer_idx in range ( 2 , self . n_layers + 1 ): # Upsample condition upsample = layer_idx > 2 # Calculate image size and channels at the layer prev_layer_size = 2 ** ( layer_idx - 1 ) layer_size : Resolution = 2 ** layer_idx layer_channel = channels [ layer_size ] # Upsampling Conv Block if upsample : self . up_convs . append ( UpModConvBlock ( channels [ prev_layer_size ], layer_channel , 3 , latent_dim , 2 , blur_kernel , ) ) # Normal Conv Block self . convs . append ( ModConvBlock ( layer_channel , layer_channel , 3 , latent_dim )) # ToRGB Block self . to_rgbs . append ( ToRGB ( layer_channel , latent_dim , 2 if upsample else 1 , blur_kernel , ) )","title":"Generator"},{"location":"all/#stylegan2_torch.generator.Generator.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"all/#stylegan2_torch.generator.Generator.convs","text":"convs = nn . ModuleList ()","title":"convs"},{"location":"all/#stylegan2_torch.generator.Generator.input","text":"input = ConstantInput ( channels [ 4 ], 4 )","title":"input"},{"location":"all/#stylegan2_torch.generator.Generator.latent_dim","text":"latent_dim = latent_dim","title":"latent_dim"},{"location":"all/#stylegan2_torch.generator.Generator.mapping","text":"mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult )","title":"mapping"},{"location":"all/#stylegan2_torch.generator.Generator.n_layers","text":"n_layers = int ( math . log ( resolution , 2 ))","title":"n_layers"},{"location":"all/#stylegan2_torch.generator.Generator.n_w_plus","text":"n_w_plus = self . n_layers * 2 - 2","title":"n_w_plus"},{"location":"all/#stylegan2_torch.generator.Generator.to_rgbs","text":"to_rgbs = nn . ModuleList ()","title":"to_rgbs"},{"location":"all/#stylegan2_torch.generator.Generator.up_convs","text":"up_convs = nn . ModuleList ()","title":"up_convs"},{"location":"all/#stylegan2_torch.generator.Generator.forward","text":"forward ( input : List [ Tensor ], * , return_latents : bool = False , input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , trunc_option : Optional [ Tuple [ float , Tensor ]] = None , mix_index : Optional [ int ] = None , noises : Optional [ List [ Optional [ Tensor ]]] = None ) Source code in stylegan2_torch/generator/__init__.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def forward ( self , # Input tensors (N, latent_dim) input : List [ Tensor ], * , # Return latents return_latents : bool = False , # Type of input tensor input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , # Truncation options trunc_option : Optional [ Tuple [ float , Tensor ]] = None , # Mixing regularization options mix_index : Optional [ int ] = None , # Noise vectors noises : Optional [ List [ Optional [ Tensor ]]] = None , ): # Get w vectors (can have 2 w vectors for mixing regularization) ws : List [ Tensor ] if input_type == \"z\" : ws = [ self . mapping ( z ) for z in input ] else : ws = input # Perform truncation if trunc_option : trunc_coeff , trunc_tensor = trunc_option ws = [ trunc_tensor + trunc_coeff * ( w - trunc_tensor ) for w in ws ] # Mixing regularization (why add dimension 1 not 0 lol) w_plus : Tensor if len ( ws ) == 1 : # No mixing regularization mix_index = self . n_w_plus if input_type == \"w_plus\" : w_plus = ws [ 0 ] else : w_plus = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) else : mix_index = mix_index if mix_index else random . randint ( 1 , self . n_w_plus - 1 ) w_plus1 = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) w_plus2 = ws [ 1 ] . unsqueeze ( 1 ) . repeat ( 1 , self . n_w_plus - mix_index , 1 ) w_plus = torch . cat ([ w_plus1 , w_plus2 ], 1 ) # Get noise noises_ : List [ Optional [ Tensor ]] = ( noises if noises else [ None ] * ( self . n_w_plus - 1 ) ) # Constant input out = self . input ( w_plus ) # References for this weird indexing: # https://github.com/NVlabs/stylegan2-ada-pytorch/issues/50 # https://github.com/rosinality/stylegan2-pytorch/issues/278 img = None for i in range ( self . n_layers - 1 ): if i > 0 : out = self . up_convs [ i - 1 ]( out , w_plus [:, i * 2 - 1 ], noises_ [ i * 2 - 1 ] ) out = self . convs [ i ]( out , w_plus [:, i * 2 ], noises_ [ i * 2 ]) img = self . to_rgbs [ i ]( out , w_plus [:, i * 2 + 1 ], img ) if return_latents : return img , w_plus else : return img","title":"forward()"},{"location":"all/#stylegan2_torch.generator.Generator.mean_latent","text":"mean_latent ( n_sample : int , device : str ) -> Tensor Source code in stylegan2_torch/generator/__init__.py 98 99 100 101 102 103 def mean_latent ( self , n_sample : int , device : str ) -> Tensor : mean_latent = self . mapping ( torch . randn ( n_sample , self . latent_dim , device = device ) ) . mean ( 0 , keepdim = True ) mean_latent . detach_ () return mean_latent","title":"mean_latent()"},{"location":"all/#stylegan2_torch.__docs","text":"__docs () Build gh-pages documentation branch. Source code in stylegan2_torch/__init__.py 48 49 50 51 52 53 54 55 def __docs (): # pragma: no cover \"\"\" Build gh-pages documentation branch. \"\"\" shell ( \"cp README.md docs/index.md && \\ mkdocs gh-deploy --force\" )","title":"__docs()"},{"location":"all/#stylegan2_torch.__serve","text":"__serve () Serve local documentation. Source code in stylegan2_torch/__init__.py 37 38 39 40 41 42 43 44 45 def __serve (): # pragma: no cover \"\"\" Serve local documentation. \"\"\" print ( \"serving\" ) shell ( \"cp README.md docs/index.md && \\ mkdocs serve\" )","title":"__serve()"},{"location":"all/#stylegan2_torch.__test","text":"__test () Runs pytest locally and keeps only coverage.xml for GitHub Actions to upload to Codecov. Source code in stylegan2_torch/__init__.py 27 28 29 30 31 32 33 34 def __test (): # pragma: no cover \"\"\" Runs pytest locally and keeps only `coverage.xml` for GitHub Actions to upload to Codecov. \"\"\" shell ( \"pytest --cov=simple_poetry --cov-report xml --cov-report term-missing tests \\ && rm -rf .pytest_cache && rm .coverage\" )","title":"__test()"},{"location":"discriminator/","text":"Discriminator Discriminator ( resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Discriminator module Source code in stylegan2_torch/discriminator/__init__.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () # FromRGB followed by ResBlock self . n_layers = int ( math . log ( resolution , 2 )) self . blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), * [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** ( i - 1 )], blur_kernel ) for i in range ( self . n_layers , 2 , - 1 ) ], ) # Minibatch std settings self . stddev_group = 4 self . stddev_feat = 1 # Final layers self . final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 ) self . final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ]) self . final_linear = EqualLinear ( channels [ 4 ], 1 ) __call__ class-attribute __call__ = proxy ( forward ) blocks instance-attribute blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** i - 1 ], blur_kernel , ) for i in range ( self . n_layers , 2 , - 1 ) ], ) final_conv instance-attribute final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 ) final_linear instance-attribute final_linear = EqualLinear ( channels [ 4 ], 1 ) final_relu instance-attribute final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ] ) n_layers instance-attribute n_layers = int ( math . log ( resolution , 2 )) stddev_feat instance-attribute stddev_feat = 1 stddev_group instance-attribute stddev_group = 4 forward forward ( input : Tensor , * , return_features : bool = False ) Source code in stylegan2_torch/discriminator/__init__.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def forward ( self , input : Tensor , * , return_features : bool = False ): # Downsampling blocks out : Tensor = self . blocks ( input ) # Minibatch stddev layer in Progressive GAN https://www.youtube.com/watch?v=V1qQXb9KcDY # Purpose is to provide variational information to the discriminator to prevent mode collapse # Other layers do not cross sample boundaries batch , channel , height , width = out . shape n_groups = min ( batch , self . stddev_group ) stddev = out . view ( n_groups , - 1 , self . stddev_feat , channel // self . stddev_feat , height , width ) stddev = torch . sqrt ( stddev . var ( 0 , unbiased = False ) + 1e-8 ) stddev = stddev . mean ([ 2 , 3 , 4 ], keepdim = True ) . squeeze ( 2 ) stddev = stddev . repeat ( n_groups , 1 , height , width ) out = torch . cat ([ out , stddev ], 1 ) # Final layers out = self . final_conv ( out ) features = self . final_relu ( out . view ( batch , - 1 )) out = self . final_linear ( features ) if return_features : return out , features else : return out blocks ConvBlock ConvBlock ( in_channel : int , out_channel : int , kernel_size : int ) Bases: nn . Sequential Convolution in feature space EqualConv2d: 2D convolution with equalized learning rate FusedLeakyReLU: LeakyReLU with a bias added before activation Source code in stylegan2_torch/discriminator/blocks.py 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int ): super () . __init__ ( EqualConv2d ( in_channel , out_channel , kernel_size , padding = kernel_size // 2 , stride = 1 , bias = False , ), FusedLeakyReLU ( out_channel , bias = True ), ) __call__ __call__ ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 32 33 def __call__ ( self , input : Tensor ) -> Tensor : return super () . __call__ ( input ) DownConvBlock DownConvBlock ( in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ) Bases: nn . Sequential Downsampling convolution in feature space Blur: Gaussian filter as low-pass filter for anti-aliasing + adjust tensor shape to preserve downsampled tensor shape EqualConv2d: 2D (downsampling) convolution with equalized learning rate FusedLeakyReLU: LeakyReLU with a bias added before activation Source code in stylegan2_torch/discriminator/blocks.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ): super () . __init__ ( Blur ( blur_kernel , - down , kernel_size ), EqualConv2d ( in_channel , out_channel , kernel_size , padding = 0 , stride = down , bias = False ), FusedLeakyReLU ( out_channel , bias = True ), ) __call__ __call__ ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 61 62 def __call__ ( self , input : Tensor ) -> Tensor : return super () . __call__ ( input ) RGBDown RGBDown ( in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ) Bases: nn . Sequential Downsampling convolution in RGB space, hence no need nonlinearity Blur: Gaussian filter as low-pass filter for anti-aliasing + adjust tensor shape to preserve downsampled tensor shape EqualConv2d: 2D (downsampling) convolution with equalized learning rate Source code in stylegan2_torch/discriminator/blocks.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ): super () . __init__ ( Blur ( blur_kernel , - down , kernel_size ), EqualConv2d ( in_channel , out_channel , kernel_size , padding = 0 , stride = down , bias = False ), ) __call__ __call__ ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 89 90 def __call__ ( self , input : Tensor ) -> Tensor : return super () . __call__ ( input ) ResBlock ResBlock ( in_channel : int , out_channel : int , blur_kernel : List [ int ], ) Bases: nn . Module Residual block ConvBlock + DownConvBlock: Convolution + downsampling RGBDown: Skip connection from higher (double) resolution RGB image Source code in stylegan2_torch/discriminator/blocks.py 101 102 103 104 105 106 107 108 def __init__ ( self , in_channel : int , out_channel : int , blur_kernel : List [ int ]): super () . __init__ () self . conv = ConvBlock ( in_channel , in_channel , 3 ) self . down_conv = DownConvBlock ( in_channel , out_channel , 3 , down = 2 , blur_kernel = blur_kernel ) self . skip = RGBDown ( in_channel , out_channel , 1 , down = 2 , blur_kernel = blur_kernel ) __call__ class-attribute __call__ = proxy ( forward ) conv instance-attribute conv = ConvBlock ( in_channel , in_channel , 3 ) down_conv instance-attribute down_conv = DownConvBlock ( in_channel , out_channel , 3 , down = 2 , blur_kernel = blur_kernel , ) skip instance-attribute skip = RGBDown ( in_channel , out_channel , 1 , down = 2 , blur_kernel = blur_kernel , ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 110 111 112 113 114 115 116 def forward ( self , input : Tensor ) -> Tensor : out = self . conv ( input ) out = self . down_conv ( out ) skip = self . skip ( input ) # sqrt 2 to adhere to equalized learning rate philosophy # (i.e. preserve variance in forward pass not initialization) return ( out + skip ) / math . sqrt ( 2 )","title":"discriminator"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator","text":"Discriminator ( resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Discriminator module Source code in stylegan2_torch/discriminator/__init__.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , resolution : Resolution , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () # FromRGB followed by ResBlock self . n_layers = int ( math . log ( resolution , 2 )) self . blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), * [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** ( i - 1 )], blur_kernel ) for i in range ( self . n_layers , 2 , - 1 ) ], ) # Minibatch std settings self . stddev_group = 4 self . stddev_feat = 1 # Final layers self . final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 ) self . final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ]) self . final_linear = EqualLinear ( channels [ 4 ], 1 )","title":"Discriminator"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.blocks","text":"blocks = nn . Sequential ( ConvBlock ( 1 , channels [ resolution ], 1 ), [ ResBlock ( channels [ 2 ** i ], channels [ 2 ** i - 1 ], blur_kernel , ) for i in range ( self . n_layers , 2 , - 1 ) ], )","title":"blocks"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.final_conv","text":"final_conv = ConvBlock ( channels [ 4 ] + 1 , channels [ 4 ], 3 )","title":"final_conv"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.final_linear","text":"final_linear = EqualLinear ( channels [ 4 ], 1 )","title":"final_linear"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.final_relu","text":"final_relu = EqualLeakyReLU ( channels [ 4 ] * 4 * 4 , channels [ 4 ] )","title":"final_relu"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.n_layers","text":"n_layers = int ( math . log ( resolution , 2 ))","title":"n_layers"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.stddev_feat","text":"stddev_feat = 1","title":"stddev_feat"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.stddev_group","text":"stddev_group = 4","title":"stddev_group"},{"location":"discriminator/#stylegan2_torch.discriminator.Discriminator.forward","text":"forward ( input : Tensor , * , return_features : bool = False ) Source code in stylegan2_torch/discriminator/__init__.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def forward ( self , input : Tensor , * , return_features : bool = False ): # Downsampling blocks out : Tensor = self . blocks ( input ) # Minibatch stddev layer in Progressive GAN https://www.youtube.com/watch?v=V1qQXb9KcDY # Purpose is to provide variational information to the discriminator to prevent mode collapse # Other layers do not cross sample boundaries batch , channel , height , width = out . shape n_groups = min ( batch , self . stddev_group ) stddev = out . view ( n_groups , - 1 , self . stddev_feat , channel // self . stddev_feat , height , width ) stddev = torch . sqrt ( stddev . var ( 0 , unbiased = False ) + 1e-8 ) stddev = stddev . mean ([ 2 , 3 , 4 ], keepdim = True ) . squeeze ( 2 ) stddev = stddev . repeat ( n_groups , 1 , height , width ) out = torch . cat ([ out , stddev ], 1 ) # Final layers out = self . final_conv ( out ) features = self . final_relu ( out . view ( batch , - 1 )) out = self . final_linear ( features ) if return_features : return out , features else : return out","title":"forward()"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks","text":"","title":"blocks"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ConvBlock","text":"ConvBlock ( in_channel : int , out_channel : int , kernel_size : int ) Bases: nn . Sequential Convolution in feature space EqualConv2d: 2D convolution with equalized learning rate FusedLeakyReLU: LeakyReLU with a bias added before activation Source code in stylegan2_torch/discriminator/blocks.py 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int ): super () . __init__ ( EqualConv2d ( in_channel , out_channel , kernel_size , padding = kernel_size // 2 , stride = 1 , bias = False , ), FusedLeakyReLU ( out_channel , bias = True ), )","title":"ConvBlock"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ConvBlock.__call__","text":"__call__ ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 32 33 def __call__ ( self , input : Tensor ) -> Tensor : return super () . __call__ ( input )","title":"__call__()"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.DownConvBlock","text":"DownConvBlock ( in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ) Bases: nn . Sequential Downsampling convolution in feature space Blur: Gaussian filter as low-pass filter for anti-aliasing + adjust tensor shape to preserve downsampled tensor shape EqualConv2d: 2D (downsampling) convolution with equalized learning rate FusedLeakyReLU: LeakyReLU with a bias added before activation Source code in stylegan2_torch/discriminator/blocks.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ): super () . __init__ ( Blur ( blur_kernel , - down , kernel_size ), EqualConv2d ( in_channel , out_channel , kernel_size , padding = 0 , stride = down , bias = False ), FusedLeakyReLU ( out_channel , bias = True ), )","title":"DownConvBlock"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.DownConvBlock.__call__","text":"__call__ ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 61 62 def __call__ ( self , input : Tensor ) -> Tensor : return super () . __call__ ( input )","title":"__call__()"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.RGBDown","text":"RGBDown ( in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ) Bases: nn . Sequential Downsampling convolution in RGB space, hence no need nonlinearity Blur: Gaussian filter as low-pass filter for anti-aliasing + adjust tensor shape to preserve downsampled tensor shape EqualConv2d: 2D (downsampling) convolution with equalized learning rate Source code in stylegan2_torch/discriminator/blocks.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , down : int , blur_kernel : List [ int ], ): super () . __init__ ( Blur ( blur_kernel , - down , kernel_size ), EqualConv2d ( in_channel , out_channel , kernel_size , padding = 0 , stride = down , bias = False ), )","title":"RGBDown"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.RGBDown.__call__","text":"__call__ ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 89 90 def __call__ ( self , input : Tensor ) -> Tensor : return super () . __call__ ( input )","title":"__call__()"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ResBlock","text":"ResBlock ( in_channel : int , out_channel : int , blur_kernel : List [ int ], ) Bases: nn . Module Residual block ConvBlock + DownConvBlock: Convolution + downsampling RGBDown: Skip connection from higher (double) resolution RGB image Source code in stylegan2_torch/discriminator/blocks.py 101 102 103 104 105 106 107 108 def __init__ ( self , in_channel : int , out_channel : int , blur_kernel : List [ int ]): super () . __init__ () self . conv = ConvBlock ( in_channel , in_channel , 3 ) self . down_conv = DownConvBlock ( in_channel , out_channel , 3 , down = 2 , blur_kernel = blur_kernel ) self . skip = RGBDown ( in_channel , out_channel , 1 , down = 2 , blur_kernel = blur_kernel )","title":"ResBlock"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ResBlock.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ResBlock.conv","text":"conv = ConvBlock ( in_channel , in_channel , 3 )","title":"conv"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ResBlock.down_conv","text":"down_conv = DownConvBlock ( in_channel , out_channel , 3 , down = 2 , blur_kernel = blur_kernel , )","title":"down_conv"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ResBlock.skip","text":"skip = RGBDown ( in_channel , out_channel , 1 , down = 2 , blur_kernel = blur_kernel , )","title":"skip"},{"location":"discriminator/#stylegan2_torch.discriminator.blocks.ResBlock.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/discriminator/blocks.py 110 111 112 113 114 115 116 def forward ( self , input : Tensor ) -> Tensor : out = self . conv ( input ) out = self . down_conv ( out ) skip = self . skip ( input ) # sqrt 2 to adhere to equalized learning rate philosophy # (i.e. preserve variance in forward pass not initialization) return ( out + skip ) / math . sqrt ( 2 )","title":"forward()"},{"location":"equalized_lr/","text":"Blur Blur ( blur_kernel : List [ int ], factor : int , kernel_size : int ) Bases: nn . Module Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Parameters: Name Type Description Default input Tensor (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H * U, W * U) Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Parameters: Name Type Description Default input Tensor (N, C, H, W) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) Source code in stylegan2_torch/equalized_lr.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , blur_kernel : List [ int ], factor : int , kernel_size : int ): \"\"\" Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Case 1: Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Args: input (Tensor): (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H * U, W * U) Case 2: Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Args: input (Tensor): (N, C, H, W) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) \"\"\" super () . __init__ () if factor > 0 : p = ( len ( blur_kernel ) - factor ) - ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 + factor - 1 pad1 = p // 2 + 1 else : p = ( len ( blur_kernel ) - abs ( factor )) + ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 pad1 = p // 2 # Factor to compensate for averaging with zeros if upsampling self . kernel : Tensor self . register_buffer ( \"kernel\" , make_kernel ( blur_kernel , factor if factor > 0 else 1 ) ) self . pad = ( pad0 , pad1 ) __call__ class-attribute __call__ = proxy ( forward ) kernel instance-attribute kernel : Tensor = None pad instance-attribute pad = ( pad0 , pad1 ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 171 172 def forward ( self , input : Tensor ) -> Tensor : return upfirdn2d ( input , self . kernel , pad = self . pad ) EqualConv2d EqualConv2d ( in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ) Bases: nn . Module Conv2d with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) ) # std = gain / sqrt(fan_in) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) self . stride = stride self . padding = padding self . bias = Parameter ( torch . zeros ( out_channel )) if bias else None __call__ class-attribute __call__ = proxy ( forward ) bias instance-attribute bias = Parameter ( torch . zeros ( out_channel )) if bias else None padding instance-attribute padding = padding scale instance-attribute scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) stride instance-attribute stride = stride weight instance-attribute weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) ) __repr__ __repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 51 52 53 54 55 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } ,\" f \" { self . weight . shape [ 2 ] } , stride= { self . stride } , padding= { self . padding } )\" ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 42 43 44 45 46 47 48 49 def forward ( self , input : Tensor ) -> Tensor : return conv2d ( input = input , weight = self . weight * self . scale , bias = self . bias , stride = self . stride , padding = self . padding , ) EqualLeakyReLU EqualLeakyReLU ( in_dim : int , out_dim : int , lr_mult : float = 1 ) Bases: nn . Module Leaky ReLU with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , in_dim : int , out_dim : int , lr_mult : float = 1 ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult __call__ class-attribute __call__ = proxy ( forward ) bias instance-attribute bias = Parameter ( torch . zeros ( out_dim )) lr_mult instance-attribute lr_mult = lr_mult scale instance-attribute scale = 1 / math . sqrt ( in_dim ) * lr_mult weight instance-attribute weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) ) __repr__ __repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 115 116 117 118 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 111 112 113 def forward ( self , input : Tensor ) -> Tensor : out = F . linear ( input , self . weight * self . scale ) return fused_leaky_relu ( out , self . bias * self . lr_mult ) EqualLinear EqualLinear ( in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ) Bases: nn . Module Linear with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult __call__ class-attribute __call__ = proxy ( forward ) bias instance-attribute bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init )) lr_mult instance-attribute lr_mult = lr_mult scale instance-attribute scale = 1 / math . sqrt ( in_dim ) * lr_mult weight instance-attribute weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) ) __repr__ __repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 86 87 88 89 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 83 84 def forward ( self , input : Tensor ) -> Tensor : return F . linear ( input , self . weight * self . scale , bias = self . bias * self . lr_mult )","title":"equalized_lr"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.Blur","text":"Blur ( blur_kernel : List [ int ], factor : int , kernel_size : int ) Bases: nn . Module Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Parameters: Name Type Description Default input Tensor (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H * U, W * U) Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Parameters: Name Type Description Default input Tensor (N, C, H, W) required blur_kernel Tensor FIR filter required factor int U. Defaults to 2. required kernel_size int K. Defaults to 3. required Returns: Name Type Description Tensor (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) Source code in stylegan2_torch/equalized_lr.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , blur_kernel : List [ int ], factor : int , kernel_size : int ): \"\"\" Apply blurring FIR filter (before / after) a (downsampling / upsampling) op Case 1: Upsample (factor > 0) Applied after a transpose convolution of stride U and kernel size K Args: input (Tensor): (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H * U, W * U) Case 2: Downsample (factor < 0) Applied before a convolution of stride U and kernel size K Args: input (Tensor): (N, C, H, W) blur_kernel (Tensor): FIR filter factor (int, optional): U. Defaults to 2. kernel_size (int, optional): K. Defaults to 3. Returns: Tensor: (N, C, H - (U + 1) + K - 1, H - (U + 1) + K - 1) \"\"\" super () . __init__ () if factor > 0 : p = ( len ( blur_kernel ) - factor ) - ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 + factor - 1 pad1 = p // 2 + 1 else : p = ( len ( blur_kernel ) - abs ( factor )) + ( kernel_size - 1 ) pad0 = ( p + 1 ) // 2 pad1 = p // 2 # Factor to compensate for averaging with zeros if upsampling self . kernel : Tensor self . register_buffer ( \"kernel\" , make_kernel ( blur_kernel , factor if factor > 0 else 1 ) ) self . pad = ( pad0 , pad1 )","title":"Blur"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.Blur.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.Blur.kernel","text":"kernel : Tensor = None","title":"kernel"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.Blur.pad","text":"pad = ( pad0 , pad1 )","title":"pad"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.Blur.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 171 172 def forward ( self , input : Tensor ) -> Tensor : return upfirdn2d ( input , self . kernel , pad = self . pad )","title":"forward()"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d","text":"EqualConv2d ( in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ) Bases: nn . Module Conv2d with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , stride : int = 1 , padding : int = 0 , bias : bool = True , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) ) # std = gain / sqrt(fan_in) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) self . stride = stride self . padding = padding self . bias = Parameter ( torch . zeros ( out_channel )) if bias else None","title":"EqualConv2d"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.bias","text":"bias = Parameter ( torch . zeros ( out_channel )) if bias else None","title":"bias"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.padding","text":"padding = padding","title":"padding"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.scale","text":"scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 )","title":"scale"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.stride","text":"stride = stride","title":"stride"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.weight","text":"weight = Parameter ( torch . randn ( out_channel , in_channel , kernel_size , kernel_size ) )","title":"weight"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.__repr__","text":"__repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 51 52 53 54 55 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } ,\" f \" { self . weight . shape [ 2 ] } , stride= { self . stride } , padding= { self . padding } )\" )","title":"__repr__()"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualConv2d.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 42 43 44 45 46 47 48 49 def forward ( self , input : Tensor ) -> Tensor : return conv2d ( input = input , weight = self . weight * self . scale , bias = self . bias , stride = self . stride , padding = self . padding , )","title":"forward()"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU","text":"EqualLeakyReLU ( in_dim : int , out_dim : int , lr_mult : float = 1 ) Bases: nn . Module Leaky ReLU with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self , in_dim : int , out_dim : int , lr_mult : float = 1 ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult","title":"EqualLeakyReLU"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU.bias","text":"bias = Parameter ( torch . zeros ( out_dim ))","title":"bias"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU.lr_mult","text":"lr_mult = lr_mult","title":"lr_mult"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU.scale","text":"scale = 1 / math . sqrt ( in_dim ) * lr_mult","title":"scale"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU.weight","text":"weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) )","title":"weight"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU.__repr__","text":"__repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 115 116 117 118 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" )","title":"__repr__()"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLeakyReLU.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 111 112 113 def forward ( self , input : Tensor ) -> Tensor : out = F . linear ( input , self . weight * self . scale ) return fused_leaky_relu ( out , self . bias * self . lr_mult )","title":"forward()"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear","text":"EqualLinear ( in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ) Bases: nn . Module Linear with equalized learning rate Source code in stylegan2_torch/equalized_lr.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_dim : int , out_dim : int , bias_init : int = 0 , lr_mult : float = 1 , ): super () . __init__ () # Equalized Learning Rate self . weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult )) self . bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init )) self . scale = ( 1 / math . sqrt ( in_dim )) * lr_mult self . lr_mult = lr_mult","title":"EqualLinear"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear.bias","text":"bias = Parameter ( torch . zeros ( out_dim ) . fill_ ( bias_init ))","title":"bias"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear.lr_mult","text":"lr_mult = lr_mult","title":"lr_mult"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear.scale","text":"scale = 1 / math . sqrt ( in_dim ) * lr_mult","title":"scale"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear.weight","text":"weight = Parameter ( torch . randn ( out_dim , in_dim ) . div_ ( lr_mult ) )","title":"weight"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear.__repr__","text":"__repr__ () -> str Source code in stylegan2_torch/equalized_lr.py 86 87 88 89 def __repr__ ( self ) -> str : return ( f \" { self . __class__ . __name__ } ( { self . weight . shape [ 1 ] } , { self . weight . shape [ 0 ] } )\" )","title":"__repr__()"},{"location":"equalized_lr/#stylegan2_torch.equalized_lr.EqualLinear.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/equalized_lr.py 83 84 def forward ( self , input : Tensor ) -> Tensor : return F . linear ( input , self . weight * self . scale , bias = self . bias * self . lr_mult )","title":"forward()"},{"location":"generator/","text":"ConstantInput ConstantInput ( channels : int , size : Resolution ) Bases: nn . Module Constant input image Source code in stylegan2_torch/generator/__init__.py 20 21 22 def __init__ ( self , channels : int , size : Resolution ): super () . __init__ () self . input = Parameter ( torch . randn ( 1 , channels , size , size )) __call__ class-attribute __call__ = proxy ( forward ) input instance-attribute input = Parameter ( torch . randn ( 1 , channels , size , size )) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/generator/__init__.py 24 25 26 def forward ( self , input : Tensor ) -> Tensor : # Broadcast constant input to each sample return self . input . repeat ( input . shape [ 0 ], 1 , 1 , 1 ) Generator Generator ( resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Generator module Source code in stylegan2_torch/generator/__init__.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () self . latent_dim = latent_dim # Create mapping network self . mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult ) # Create constant input self . input = ConstantInput ( channels [ 4 ], 4 ) # Create Conv, UpConv and ToRGB Blocks self . convs = nn . ModuleList () self . up_convs = nn . ModuleList () self . to_rgbs = nn . ModuleList () self . n_layers = int ( math . log ( resolution , 2 )) self . n_w_plus = self . n_layers * 2 - 2 for layer_idx in range ( 2 , self . n_layers + 1 ): # Upsample condition upsample = layer_idx > 2 # Calculate image size and channels at the layer prev_layer_size = 2 ** ( layer_idx - 1 ) layer_size : Resolution = 2 ** layer_idx layer_channel = channels [ layer_size ] # Upsampling Conv Block if upsample : self . up_convs . append ( UpModConvBlock ( channels [ prev_layer_size ], layer_channel , 3 , latent_dim , 2 , blur_kernel , ) ) # Normal Conv Block self . convs . append ( ModConvBlock ( layer_channel , layer_channel , 3 , latent_dim )) # ToRGB Block self . to_rgbs . append ( ToRGB ( layer_channel , latent_dim , 2 if upsample else 1 , blur_kernel , ) ) __call__ class-attribute __call__ = proxy ( forward ) convs instance-attribute convs = nn . ModuleList () input instance-attribute input = ConstantInput ( channels [ 4 ], 4 ) latent_dim instance-attribute latent_dim = latent_dim mapping instance-attribute mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult ) n_layers instance-attribute n_layers = int ( math . log ( resolution , 2 )) n_w_plus instance-attribute n_w_plus = self . n_layers * 2 - 2 to_rgbs instance-attribute to_rgbs = nn . ModuleList () up_convs instance-attribute up_convs = nn . ModuleList () forward forward ( input : List [ Tensor ], * , return_latents : bool = False , input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , trunc_option : Optional [ Tuple [ float , Tensor ]] = None , mix_index : Optional [ int ] = None , noises : Optional [ List [ Optional [ Tensor ]]] = None ) Source code in stylegan2_torch/generator/__init__.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def forward ( self , # Input tensors (N, latent_dim) input : List [ Tensor ], * , # Return latents return_latents : bool = False , # Type of input tensor input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , # Truncation options trunc_option : Optional [ Tuple [ float , Tensor ]] = None , # Mixing regularization options mix_index : Optional [ int ] = None , # Noise vectors noises : Optional [ List [ Optional [ Tensor ]]] = None , ): # Get w vectors (can have 2 w vectors for mixing regularization) ws : List [ Tensor ] if input_type == \"z\" : ws = [ self . mapping ( z ) for z in input ] else : ws = input # Perform truncation if trunc_option : trunc_coeff , trunc_tensor = trunc_option ws = [ trunc_tensor + trunc_coeff * ( w - trunc_tensor ) for w in ws ] # Mixing regularization (why add dimension 1 not 0 lol) w_plus : Tensor if len ( ws ) == 1 : # No mixing regularization mix_index = self . n_w_plus if input_type == \"w_plus\" : w_plus = ws [ 0 ] else : w_plus = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) else : mix_index = mix_index if mix_index else random . randint ( 1 , self . n_w_plus - 1 ) w_plus1 = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) w_plus2 = ws [ 1 ] . unsqueeze ( 1 ) . repeat ( 1 , self . n_w_plus - mix_index , 1 ) w_plus = torch . cat ([ w_plus1 , w_plus2 ], 1 ) # Get noise noises_ : List [ Optional [ Tensor ]] = ( noises if noises else [ None ] * ( self . n_w_plus - 1 ) ) # Constant input out = self . input ( w_plus ) # References for this weird indexing: # https://github.com/NVlabs/stylegan2-ada-pytorch/issues/50 # https://github.com/rosinality/stylegan2-pytorch/issues/278 img = None for i in range ( self . n_layers - 1 ): if i > 0 : out = self . up_convs [ i - 1 ]( out , w_plus [:, i * 2 - 1 ], noises_ [ i * 2 - 1 ] ) out = self . convs [ i ]( out , w_plus [:, i * 2 ], noises_ [ i * 2 ]) img = self . to_rgbs [ i ]( out , w_plus [:, i * 2 + 1 ], img ) if return_latents : return img , w_plus else : return img mean_latent mean_latent ( n_sample : int , device : str ) -> Tensor Source code in stylegan2_torch/generator/__init__.py 98 99 100 101 102 103 def mean_latent ( self , n_sample : int , device : str ) -> Tensor : mean_latent = self . mapping ( torch . randn ( n_sample , self . latent_dim , device = device ) ) . mean ( 0 , keepdim = True ) mean_latent . detach_ () return mean_latent conv_block AddNoise AddNoise () Bases: nn . Module Inject white noise scaled by a learnable scalar (same noise for whole batch) Source code in stylegan2_torch/generator/conv_block.py 74 75 76 77 78 def __init__ ( self ): super () . __init__ () # Trainable parameters self . weight = Parameter ( torch . zeros ( 1 )) __call__ class-attribute __call__ = proxy ( forward ) weight instance-attribute weight = Parameter ( torch . zeros ( 1 )) forward forward ( input : Tensor , noise : Optional [ Tensor ]) -> Tensor Source code in stylegan2_torch/generator/conv_block.py 80 81 82 83 84 85 def forward ( self , input : Tensor , noise : Optional [ Tensor ]) -> Tensor : if noise is None : batch , _ , height , width = input . shape noise = input . new_empty ( batch , 1 , height , width ) . normal_ () return input + self . weight * noise ModConvBlock ModConvBlock ( in_channel : int , out_channel : int , kernel_size : int , latent_dim : int , ) Bases: nn . Module Modulated convolution block disentangled latent vector (w) => affine transformation => style vector style vector => modulate + demodulate convolution weights => new conv weights new conv weights & input features => group convolution => output features output features => add noise & leaky ReLU => final output features Source code in stylegan2_torch/generator/conv_block.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , latent_dim : int ): super () . __init__ () # Affine mapping from W to style vector self . affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) # Trainable parameters self . weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) ) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) # Noise and Leaky ReLU self . add_noise = AddNoise () self . leaky_relu = FusedLeakyReLU ( out_channel ) __call__ class-attribute __call__ = proxy ( forward ) add_noise instance-attribute add_noise = AddNoise () affine instance-attribute affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) leaky_relu instance-attribute leaky_relu = FusedLeakyReLU ( out_channel ) scale instance-attribute scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) weight instance-attribute weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) ) forward forward ( input : Tensor , w : Tensor , noise : Optional [ Tensor ] ) -> Tensor Source code in stylegan2_torch/generator/conv_block.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def forward ( self , input : Tensor , w : Tensor , noise : Optional [ Tensor ]) -> Tensor : batch , in_channel , _ , _ = input . shape # Get style vectors (N, 1, C_in, 1, 1) style = self . affine ( w ) . view ( batch , 1 , in_channel , 1 , 1 ) # Modulate weights with equalized learning rate (N, C_out, C_in, K_h, K_w) weight = mod ( self . scale * self . weight , style ) # Demodulate weights weight = demod ( weight ) # Perform convolution out = group_conv ( input , weight ) # Add noise out = self . add_noise ( out , noise ) # Add learnable bias and activate return self . leaky_relu ( out ) UpModConvBlock UpModConvBlock ( in_channel : int , out_channel : int , kernel_size : int , latent_dim : int , up : int , blur_kernel : List [ int ], ) Bases: nn . Module Modulated convolution block with upsampling disentangled latent vector (w) => affine transformation => style vector style vector => modulate + demodulate convolution weights => new conv weights new conv weights & input features => group convolution and upsampling => output features output features => add noise & leaky ReLU => final output features Source code in stylegan2_torch/generator/conv_block.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , latent_dim : int , up : int , blur_kernel : List [ int ], ): super () . __init__ () # Affine mapping from W to style vector self . affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) # Trainable parameters self . weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) ) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) # Blurring kernel self . up = up self . blur = Blur ( blur_kernel , up , kernel_size ) # Noise and Leaky ReLU self . add_noise = AddNoise () self . leaky_relu = FusedLeakyReLU ( out_channel ) __call__ class-attribute __call__ = proxy ( forward ) add_noise instance-attribute add_noise = AddNoise () affine instance-attribute affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) blur instance-attribute blur = Blur ( blur_kernel , up , kernel_size ) leaky_relu instance-attribute leaky_relu = FusedLeakyReLU ( out_channel ) scale instance-attribute scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) up instance-attribute up = up weight instance-attribute weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) ) forward forward ( input : Tensor , w : Tensor , noise : Optional [ Tensor ] ) -> Tensor Source code in stylegan2_torch/generator/conv_block.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def forward ( self , input : Tensor , w : Tensor , noise : Optional [ Tensor ]) -> Tensor : batch , in_channel , _ , _ = input . shape # Get style vectors (N, 1, C_in, 1, 1) style = self . affine ( w ) . view ( batch , 1 , in_channel , 1 , 1 ) # Modulate weights with equalized learning rate (N, C_out, C_in, K_h, K_w) weight = mod ( self . scale * self . weight , style ) # Demodulate weights weight = demod ( weight ) # Reshape to use group convolution out = group_conv_up ( input , weight , self . up ) # Apply blurring filter for anti-aliasing (linear operation so order doesn't matter?) out = self . blur ( out ) # Add noise out = self . add_noise ( out , noise ) # Add learnable bias and activate return self . leaky_relu ( out ) demod demod ( weight : Tensor ) -> Tensor Demodulate convolution weights (normalization = statistically restore output feature map to unit s.d.) Parameters: Name Type Description Default weight Tensor (N, C_out, C_in, K_h, K_w) required Returns: Name Type Description Tensor Tensor (N, C_out, C_in, K_h, K_w) Source code in stylegan2_torch/generator/conv_block.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def demod ( weight : Tensor ) -> Tensor : \"\"\" Demodulate convolution weights (normalization = statistically restore output feature map to unit s.d.) Args: weight (Tensor): (N, C_out, C_in, K_h, K_w) Returns: Tensor: (N, C_out, C_in, K_h, K_w) \"\"\" batch , out_channel , _ , _ , _ = weight . shape demod = torch . rsqrt ( weight . pow ( 2 ) . sum ([ 2 , 3 , 4 ]) + 1e-8 ) . view ( batch , out_channel , 1 , 1 , 1 ) return weight * demod group_conv group_conv ( input : Tensor , weight : Tensor ) -> Tensor Efficiently perform modulated convolution (i.e. grouped convolution) Parameters: Name Type Description Default input Tensor (N, C_in, H, W) required weight Tensor (N, C_out, C_in, K, K) required Returns: Name Type Description Tensor Tensor (N, C, H + K - 1, W + K - 1) Source code in stylegan2_torch/generator/conv_block.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def group_conv ( input : Tensor , weight : Tensor ) -> Tensor : \"\"\" Efficiently perform modulated convolution (i.e. grouped convolution) Args: input (Tensor): (N, C_in, H, W) weight (Tensor): (N, C_out, C_in, K, K) Returns: Tensor: (N, C, H + K - 1, W + K - 1) \"\"\" batch , in_channel , height , width = input . shape _ , out_channel , _ , k_h , k_w = weight . shape weight = weight . view ( batch * out_channel , in_channel , k_h , k_w ) input = input . view ( 1 , batch * in_channel , height , width ) out = conv2d ( input = input , weight = weight , padding = k_h // 2 , groups = batch ) return out . view ( batch , out_channel , height , width ) group_conv_up group_conv_up ( input : Tensor , weight : Tensor , up : int = 2 ) -> Tensor Efficiently perform upsampling + modulated convolution (i.e. grouped transpose convolution) Parameters: Name Type Description Default input Tensor (N, C_in, H, W) required weight Tensor (N, C_out, C_in, K, K) required up int U. Defaults to 2. 2 Returns: Name Type Description Tensor Tensor (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) Source code in stylegan2_torch/generator/conv_block.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def group_conv_up ( input : Tensor , weight : Tensor , up : int = 2 ) -> Tensor : \"\"\" Efficiently perform upsampling + modulated convolution (i.e. grouped transpose convolution) Args: input (Tensor): (N, C_in, H, W) weight (Tensor): (N, C_out, C_in, K, K) up (int, optional): U. Defaults to 2. Returns: Tensor: (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) \"\"\" batch , in_channel , height , width = input . shape _ , out_channel , _ , k_h , k_w = weight . shape weight = weight . transpose ( 1 , 2 ) . reshape ( batch * in_channel , out_channel , k_h , k_w ) input = input . view ( 1 , batch * in_channel , height , width ) out = conv_transpose2d ( input = input , weight = weight , stride = up , padding = 0 , groups = batch ) _ , _ , out_h , out_w = out . shape return out . view ( batch , out_channel , out_h , out_w ) mod mod ( weight : Tensor , style : Tensor ) -> Tensor Modulate convolution weights with style vector (styling = scale each input feature map before convolution) Parameters: Name Type Description Default weight Tensor (1, C_out, C_in, K_h, K_w) required style Tensor (N, 1, C_in, 1, 1) required Returns: Name Type Description Tensor Tensor (N, C_out, C_in, K_h, K_w) Source code in stylegan2_torch/generator/conv_block.py 14 15 16 17 18 19 20 21 22 23 24 25 26 def mod ( weight : Tensor , style : Tensor ) -> Tensor : \"\"\" Modulate convolution weights with style vector (styling = scale each input feature map before convolution) Args: weight (Tensor): (1, C_out, C_in, K_h, K_w) style (Tensor): (N, 1, C_in, 1, 1) Returns: Tensor: (N, C_out, C_in, K_h, K_w) \"\"\" return weight * style mapping MappingNetwork MappingNetwork ( latent_dim : int , n_mlp : int , lr_mlp_mult : float ) Bases: nn . Sequential Mapping network from sampling space (z) to disentangled latent space (w) Source code in stylegan2_torch/generator/mapping.py 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , latent_dim : int , n_mlp : int , lr_mlp_mult : float ): super () . __init__ ( Normalize (), * [ EqualLeakyReLU ( latent_dim , latent_dim , lr_mult = lr_mlp_mult , ) for _ in range ( n_mlp ) ] ) Normalize Bases: nn . Module Normalize latent vector for each sample forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/generator/mapping.py 12 13 14 15 def forward ( self , input : Tensor ) -> Tensor : # input: (N, style_dim) # Normalize z in each sample to N(0,1) return input * torch . rsqrt ( torch . mean ( input ** 2 , dim = 1 , keepdim = True ) + 1e-8 ) rgb ToRGB ToRGB ( in_channel : int , latent_dim : int , up : int , blur_kernel : List [ int ], ) Bases: nn . Module Source code in stylegan2_torch/generator/rgb.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , in_channel : int , latent_dim : int , up : int , blur_kernel : List [ int ], ): super () . __init__ () # Affine mapping from W to style vector self . affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) # Trainable parameters self . weight = Parameter ( torch . randn ( 1 , 1 , in_channel , 1 , 1 )) self . scale = 1 / math . sqrt ( in_channel ) self . bias = Parameter ( torch . zeros ( 1 , 1 , 1 , 1 )) if up > 1 : self . upsample = Upsample ( blur_kernel , up ) affine instance-attribute affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) bias instance-attribute bias = Parameter ( torch . zeros ( 1 , 1 , 1 , 1 )) scale instance-attribute scale = 1 / math . sqrt ( in_channel ) upsample instance-attribute upsample = Upsample ( blur_kernel , up ) weight instance-attribute weight = Parameter ( torch . randn ( 1 , 1 , in_channel , 1 , 1 )) forward forward ( input : Tensor , w : Tensor , prev_output : Optional [ Tensor ] = None , ) -> Tensor Source code in stylegan2_torch/generator/rgb.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def forward ( self , input : Tensor , w : Tensor , prev_output : Optional [ Tensor ] = None ) -> Tensor : batch , in_channel , _ , _ = input . shape # Get style vectors (N, 1, C_in, 1, 1) style = self . affine ( w ) . view ( batch , 1 , in_channel , 1 , 1 ) # Modulate weights with equalized learning rate (N, C_out, C_in, K_h, K_w) weight = mod ( self . scale * self . weight , style ) # Perform convolution and add bias out = group_conv ( input , weight ) + self . bias if prev_output is not None : out = out + self . upsample ( prev_output ) return out Upsample Upsample ( blur_kernel : List [ int ], factor : int ) Bases: nn . Module Upsampling + apply blurring FIR filter Source code in stylegan2_torch/generator/rgb.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , blur_kernel : List [ int ], factor : int ): super () . __init__ () self . factor = factor # Factor to compensate for averaging with zeros self . kernel : Tensor self . register_buffer ( \"kernel\" , make_kernel ( blur_kernel , self . factor )) # Since upsampling by factor means there is factor - 1 pad1 already built-in \"\"\" UPSAMPLE CASE kernel: [kkkkk]................[kkkkk] (k_w = 5) upsampled: [x---x---x---x---x---x---] (in_w = 6, up_x = 4) padded: [ppppx---x---x---x---x---x---] (pad0 = 4, pad1 = 0) output: [oooooooooooooooooooooooo] (out_w = 24) Hence, pad0 + pad1 = k_w - 1 pad0 - pad1 = up_x - 1 DOWNSAMPLE CASE kernel: [kkkkk]...............[kkkkk] (k_w = 5) input: [xxxxxxxxxxxxxxxxxxxxxxxx] (in_w = 24) padded: [ppxxxxxxxxxxxxxxxxxxxxxxxxp] (pad0 = 2, pad1 = 1) output: [o-o-o-o-o-o-o-o-o-o-o-o] (out_w = 12) Since last (factor - 1) elements are discarded anyway, they don't need to be padded Hence, pad0 + pad1 = k_w - 1 - (factor - 1) pad0 - pad1 = 0 or 1 \"\"\" p = len ( blur_kernel ) - factor pad0 = ( p + 1 ) // 2 + factor - 1 pad1 = p // 2 self . pad = ( pad0 , pad1 ) factor instance-attribute factor = factor kernel instance-attribute kernel : Tensor = None pad instance-attribute pad = ( pad0 , pad1 ) forward forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/generator/rgb.py 57 58 59 60 61 62 def forward ( self , input : Tensor ) -> Tensor : return upfirdn2d ( input , self . kernel , up = self . factor , down = 1 , pad = self . pad )","title":"generator"},{"location":"generator/#stylegan2_torch.generator.ConstantInput","text":"ConstantInput ( channels : int , size : Resolution ) Bases: nn . Module Constant input image Source code in stylegan2_torch/generator/__init__.py 20 21 22 def __init__ ( self , channels : int , size : Resolution ): super () . __init__ () self . input = Parameter ( torch . randn ( 1 , channels , size , size ))","title":"ConstantInput"},{"location":"generator/#stylegan2_torch.generator.ConstantInput.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"generator/#stylegan2_torch.generator.ConstantInput.input","text":"input = Parameter ( torch . randn ( 1 , channels , size , size ))","title":"input"},{"location":"generator/#stylegan2_torch.generator.ConstantInput.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/generator/__init__.py 24 25 26 def forward ( self , input : Tensor ) -> Tensor : # Broadcast constant input to each sample return self . input . repeat ( input . shape [ 0 ], 1 , 1 , 1 )","title":"forward()"},{"location":"generator/#stylegan2_torch.generator.Generator","text":"Generator ( resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ) Bases: nn . Module Generator module Source code in stylegan2_torch/generator/__init__.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , resolution : Resolution , latent_dim : int = 512 , n_mlp : int = 8 , lr_mlp_mult : float = 0.01 , channels : Dict [ Resolution , int ] = default_channels , blur_kernel : List [ int ] = [ 1 , 3 , 3 , 1 ], ): super () . __init__ () self . latent_dim = latent_dim # Create mapping network self . mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult ) # Create constant input self . input = ConstantInput ( channels [ 4 ], 4 ) # Create Conv, UpConv and ToRGB Blocks self . convs = nn . ModuleList () self . up_convs = nn . ModuleList () self . to_rgbs = nn . ModuleList () self . n_layers = int ( math . log ( resolution , 2 )) self . n_w_plus = self . n_layers * 2 - 2 for layer_idx in range ( 2 , self . n_layers + 1 ): # Upsample condition upsample = layer_idx > 2 # Calculate image size and channels at the layer prev_layer_size = 2 ** ( layer_idx - 1 ) layer_size : Resolution = 2 ** layer_idx layer_channel = channels [ layer_size ] # Upsampling Conv Block if upsample : self . up_convs . append ( UpModConvBlock ( channels [ prev_layer_size ], layer_channel , 3 , latent_dim , 2 , blur_kernel , ) ) # Normal Conv Block self . convs . append ( ModConvBlock ( layer_channel , layer_channel , 3 , latent_dim )) # ToRGB Block self . to_rgbs . append ( ToRGB ( layer_channel , latent_dim , 2 if upsample else 1 , blur_kernel , ) )","title":"Generator"},{"location":"generator/#stylegan2_torch.generator.Generator.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"generator/#stylegan2_torch.generator.Generator.convs","text":"convs = nn . ModuleList ()","title":"convs"},{"location":"generator/#stylegan2_torch.generator.Generator.input","text":"input = ConstantInput ( channels [ 4 ], 4 )","title":"input"},{"location":"generator/#stylegan2_torch.generator.Generator.latent_dim","text":"latent_dim = latent_dim","title":"latent_dim"},{"location":"generator/#stylegan2_torch.generator.Generator.mapping","text":"mapping = MappingNetwork ( latent_dim , n_mlp , lr_mlp_mult )","title":"mapping"},{"location":"generator/#stylegan2_torch.generator.Generator.n_layers","text":"n_layers = int ( math . log ( resolution , 2 ))","title":"n_layers"},{"location":"generator/#stylegan2_torch.generator.Generator.n_w_plus","text":"n_w_plus = self . n_layers * 2 - 2","title":"n_w_plus"},{"location":"generator/#stylegan2_torch.generator.Generator.to_rgbs","text":"to_rgbs = nn . ModuleList ()","title":"to_rgbs"},{"location":"generator/#stylegan2_torch.generator.Generator.up_convs","text":"up_convs = nn . ModuleList ()","title":"up_convs"},{"location":"generator/#stylegan2_torch.generator.Generator.forward","text":"forward ( input : List [ Tensor ], * , return_latents : bool = False , input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , trunc_option : Optional [ Tuple [ float , Tensor ]] = None , mix_index : Optional [ int ] = None , noises : Optional [ List [ Optional [ Tensor ]]] = None ) Source code in stylegan2_torch/generator/__init__.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def forward ( self , # Input tensors (N, latent_dim) input : List [ Tensor ], * , # Return latents return_latents : bool = False , # Type of input tensor input_type : Literal [ \"z\" , \"w\" , \"w_plus\" ] = \"z\" , # Truncation options trunc_option : Optional [ Tuple [ float , Tensor ]] = None , # Mixing regularization options mix_index : Optional [ int ] = None , # Noise vectors noises : Optional [ List [ Optional [ Tensor ]]] = None , ): # Get w vectors (can have 2 w vectors for mixing regularization) ws : List [ Tensor ] if input_type == \"z\" : ws = [ self . mapping ( z ) for z in input ] else : ws = input # Perform truncation if trunc_option : trunc_coeff , trunc_tensor = trunc_option ws = [ trunc_tensor + trunc_coeff * ( w - trunc_tensor ) for w in ws ] # Mixing regularization (why add dimension 1 not 0 lol) w_plus : Tensor if len ( ws ) == 1 : # No mixing regularization mix_index = self . n_w_plus if input_type == \"w_plus\" : w_plus = ws [ 0 ] else : w_plus = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) else : mix_index = mix_index if mix_index else random . randint ( 1 , self . n_w_plus - 1 ) w_plus1 = ws [ 0 ] . unsqueeze ( 1 ) . repeat ( 1 , mix_index , 1 ) w_plus2 = ws [ 1 ] . unsqueeze ( 1 ) . repeat ( 1 , self . n_w_plus - mix_index , 1 ) w_plus = torch . cat ([ w_plus1 , w_plus2 ], 1 ) # Get noise noises_ : List [ Optional [ Tensor ]] = ( noises if noises else [ None ] * ( self . n_w_plus - 1 ) ) # Constant input out = self . input ( w_plus ) # References for this weird indexing: # https://github.com/NVlabs/stylegan2-ada-pytorch/issues/50 # https://github.com/rosinality/stylegan2-pytorch/issues/278 img = None for i in range ( self . n_layers - 1 ): if i > 0 : out = self . up_convs [ i - 1 ]( out , w_plus [:, i * 2 - 1 ], noises_ [ i * 2 - 1 ] ) out = self . convs [ i ]( out , w_plus [:, i * 2 ], noises_ [ i * 2 ]) img = self . to_rgbs [ i ]( out , w_plus [:, i * 2 + 1 ], img ) if return_latents : return img , w_plus else : return img","title":"forward()"},{"location":"generator/#stylegan2_torch.generator.Generator.mean_latent","text":"mean_latent ( n_sample : int , device : str ) -> Tensor Source code in stylegan2_torch/generator/__init__.py 98 99 100 101 102 103 def mean_latent ( self , n_sample : int , device : str ) -> Tensor : mean_latent = self . mapping ( torch . randn ( n_sample , self . latent_dim , device = device ) ) . mean ( 0 , keepdim = True ) mean_latent . detach_ () return mean_latent","title":"mean_latent()"},{"location":"generator/#stylegan2_torch.generator.conv_block","text":"","title":"conv_block"},{"location":"generator/#stylegan2_torch.generator.conv_block.AddNoise","text":"AddNoise () Bases: nn . Module Inject white noise scaled by a learnable scalar (same noise for whole batch) Source code in stylegan2_torch/generator/conv_block.py 74 75 76 77 78 def __init__ ( self ): super () . __init__ () # Trainable parameters self . weight = Parameter ( torch . zeros ( 1 ))","title":"AddNoise"},{"location":"generator/#stylegan2_torch.generator.conv_block.AddNoise.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"generator/#stylegan2_torch.generator.conv_block.AddNoise.weight","text":"weight = Parameter ( torch . zeros ( 1 ))","title":"weight"},{"location":"generator/#stylegan2_torch.generator.conv_block.AddNoise.forward","text":"forward ( input : Tensor , noise : Optional [ Tensor ]) -> Tensor Source code in stylegan2_torch/generator/conv_block.py 80 81 82 83 84 85 def forward ( self , input : Tensor , noise : Optional [ Tensor ]) -> Tensor : if noise is None : batch , _ , height , width = input . shape noise = input . new_empty ( batch , 1 , height , width ) . normal_ () return input + self . weight * noise","title":"forward()"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock","text":"ModConvBlock ( in_channel : int , out_channel : int , kernel_size : int , latent_dim : int , ) Bases: nn . Module Modulated convolution block disentangled latent vector (w) => affine transformation => style vector style vector => modulate + demodulate convolution weights => new conv weights new conv weights & input features => group convolution => output features output features => add noise & leaky ReLU => final output features Source code in stylegan2_torch/generator/conv_block.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , latent_dim : int ): super () . __init__ () # Affine mapping from W to style vector self . affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) # Trainable parameters self . weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) ) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) # Noise and Leaky ReLU self . add_noise = AddNoise () self . leaky_relu = FusedLeakyReLU ( out_channel )","title":"ModConvBlock"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock.add_noise","text":"add_noise = AddNoise ()","title":"add_noise"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock.affine","text":"affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 )","title":"affine"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock.leaky_relu","text":"leaky_relu = FusedLeakyReLU ( out_channel )","title":"leaky_relu"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock.scale","text":"scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 )","title":"scale"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock.weight","text":"weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) )","title":"weight"},{"location":"generator/#stylegan2_torch.generator.conv_block.ModConvBlock.forward","text":"forward ( input : Tensor , w : Tensor , noise : Optional [ Tensor ] ) -> Tensor Source code in stylegan2_torch/generator/conv_block.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def forward ( self , input : Tensor , w : Tensor , noise : Optional [ Tensor ]) -> Tensor : batch , in_channel , _ , _ = input . shape # Get style vectors (N, 1, C_in, 1, 1) style = self . affine ( w ) . view ( batch , 1 , in_channel , 1 , 1 ) # Modulate weights with equalized learning rate (N, C_out, C_in, K_h, K_w) weight = mod ( self . scale * self . weight , style ) # Demodulate weights weight = demod ( weight ) # Perform convolution out = group_conv ( input , weight ) # Add noise out = self . add_noise ( out , noise ) # Add learnable bias and activate return self . leaky_relu ( out )","title":"forward()"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock","text":"UpModConvBlock ( in_channel : int , out_channel : int , kernel_size : int , latent_dim : int , up : int , blur_kernel : List [ int ], ) Bases: nn . Module Modulated convolution block with upsampling disentangled latent vector (w) => affine transformation => style vector style vector => modulate + demodulate convolution weights => new conv weights new conv weights & input features => group convolution and upsampling => output features output features => add noise & leaky ReLU => final output features Source code in stylegan2_torch/generator/conv_block.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def __init__ ( self , in_channel : int , out_channel : int , kernel_size : int , latent_dim : int , up : int , blur_kernel : List [ int ], ): super () . __init__ () # Affine mapping from W to style vector self . affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) # Trainable parameters self . weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) ) self . scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 ) # Blurring kernel self . up = up self . blur = Blur ( blur_kernel , up , kernel_size ) # Noise and Leaky ReLU self . add_noise = AddNoise () self . leaky_relu = FusedLeakyReLU ( out_channel )","title":"UpModConvBlock"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.__call__","text":"__call__ = proxy ( forward )","title":"__call__"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.add_noise","text":"add_noise = AddNoise ()","title":"add_noise"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.affine","text":"affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 )","title":"affine"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.blur","text":"blur = Blur ( blur_kernel , up , kernel_size )","title":"blur"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.leaky_relu","text":"leaky_relu = FusedLeakyReLU ( out_channel )","title":"leaky_relu"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.scale","text":"scale = 1 / math . sqrt ( in_channel * kernel_size ** 2 )","title":"scale"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.up","text":"up = up","title":"up"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.weight","text":"weight = Parameter ( torch . randn ( 1 , out_channel , in_channel , kernel_size , kernel_size ) )","title":"weight"},{"location":"generator/#stylegan2_torch.generator.conv_block.UpModConvBlock.forward","text":"forward ( input : Tensor , w : Tensor , noise : Optional [ Tensor ] ) -> Tensor Source code in stylegan2_torch/generator/conv_block.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def forward ( self , input : Tensor , w : Tensor , noise : Optional [ Tensor ]) -> Tensor : batch , in_channel , _ , _ = input . shape # Get style vectors (N, 1, C_in, 1, 1) style = self . affine ( w ) . view ( batch , 1 , in_channel , 1 , 1 ) # Modulate weights with equalized learning rate (N, C_out, C_in, K_h, K_w) weight = mod ( self . scale * self . weight , style ) # Demodulate weights weight = demod ( weight ) # Reshape to use group convolution out = group_conv_up ( input , weight , self . up ) # Apply blurring filter for anti-aliasing (linear operation so order doesn't matter?) out = self . blur ( out ) # Add noise out = self . add_noise ( out , noise ) # Add learnable bias and activate return self . leaky_relu ( out )","title":"forward()"},{"location":"generator/#stylegan2_torch.generator.conv_block.demod","text":"demod ( weight : Tensor ) -> Tensor Demodulate convolution weights (normalization = statistically restore output feature map to unit s.d.) Parameters: Name Type Description Default weight Tensor (N, C_out, C_in, K_h, K_w) required Returns: Name Type Description Tensor Tensor (N, C_out, C_in, K_h, K_w) Source code in stylegan2_torch/generator/conv_block.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def demod ( weight : Tensor ) -> Tensor : \"\"\" Demodulate convolution weights (normalization = statistically restore output feature map to unit s.d.) Args: weight (Tensor): (N, C_out, C_in, K_h, K_w) Returns: Tensor: (N, C_out, C_in, K_h, K_w) \"\"\" batch , out_channel , _ , _ , _ = weight . shape demod = torch . rsqrt ( weight . pow ( 2 ) . sum ([ 2 , 3 , 4 ]) + 1e-8 ) . view ( batch , out_channel , 1 , 1 , 1 ) return weight * demod","title":"demod()"},{"location":"generator/#stylegan2_torch.generator.conv_block.group_conv","text":"group_conv ( input : Tensor , weight : Tensor ) -> Tensor Efficiently perform modulated convolution (i.e. grouped convolution) Parameters: Name Type Description Default input Tensor (N, C_in, H, W) required weight Tensor (N, C_out, C_in, K, K) required Returns: Name Type Description Tensor Tensor (N, C, H + K - 1, W + K - 1) Source code in stylegan2_torch/generator/conv_block.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def group_conv ( input : Tensor , weight : Tensor ) -> Tensor : \"\"\" Efficiently perform modulated convolution (i.e. grouped convolution) Args: input (Tensor): (N, C_in, H, W) weight (Tensor): (N, C_out, C_in, K, K) Returns: Tensor: (N, C, H + K - 1, W + K - 1) \"\"\" batch , in_channel , height , width = input . shape _ , out_channel , _ , k_h , k_w = weight . shape weight = weight . view ( batch * out_channel , in_channel , k_h , k_w ) input = input . view ( 1 , batch * in_channel , height , width ) out = conv2d ( input = input , weight = weight , padding = k_h // 2 , groups = batch ) return out . view ( batch , out_channel , height , width )","title":"group_conv()"},{"location":"generator/#stylegan2_torch.generator.conv_block.group_conv_up","text":"group_conv_up ( input : Tensor , weight : Tensor , up : int = 2 ) -> Tensor Efficiently perform upsampling + modulated convolution (i.e. grouped transpose convolution) Parameters: Name Type Description Default input Tensor (N, C_in, H, W) required weight Tensor (N, C_out, C_in, K, K) required up int U. Defaults to 2. 2 Returns: Name Type Description Tensor Tensor (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) Source code in stylegan2_torch/generator/conv_block.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def group_conv_up ( input : Tensor , weight : Tensor , up : int = 2 ) -> Tensor : \"\"\" Efficiently perform upsampling + modulated convolution (i.e. grouped transpose convolution) Args: input (Tensor): (N, C_in, H, W) weight (Tensor): (N, C_out, C_in, K, K) up (int, optional): U. Defaults to 2. Returns: Tensor: (N, C, (H - 1) * U + K - 1 + 1, (W - 1) * U + K - 1 + 1) \"\"\" batch , in_channel , height , width = input . shape _ , out_channel , _ , k_h , k_w = weight . shape weight = weight . transpose ( 1 , 2 ) . reshape ( batch * in_channel , out_channel , k_h , k_w ) input = input . view ( 1 , batch * in_channel , height , width ) out = conv_transpose2d ( input = input , weight = weight , stride = up , padding = 0 , groups = batch ) _ , _ , out_h , out_w = out . shape return out . view ( batch , out_channel , out_h , out_w )","title":"group_conv_up()"},{"location":"generator/#stylegan2_torch.generator.conv_block.mod","text":"mod ( weight : Tensor , style : Tensor ) -> Tensor Modulate convolution weights with style vector (styling = scale each input feature map before convolution) Parameters: Name Type Description Default weight Tensor (1, C_out, C_in, K_h, K_w) required style Tensor (N, 1, C_in, 1, 1) required Returns: Name Type Description Tensor Tensor (N, C_out, C_in, K_h, K_w) Source code in stylegan2_torch/generator/conv_block.py 14 15 16 17 18 19 20 21 22 23 24 25 26 def mod ( weight : Tensor , style : Tensor ) -> Tensor : \"\"\" Modulate convolution weights with style vector (styling = scale each input feature map before convolution) Args: weight (Tensor): (1, C_out, C_in, K_h, K_w) style (Tensor): (N, 1, C_in, 1, 1) Returns: Tensor: (N, C_out, C_in, K_h, K_w) \"\"\" return weight * style","title":"mod()"},{"location":"generator/#stylegan2_torch.generator.mapping","text":"","title":"mapping"},{"location":"generator/#stylegan2_torch.generator.mapping.MappingNetwork","text":"MappingNetwork ( latent_dim : int , n_mlp : int , lr_mlp_mult : float ) Bases: nn . Sequential Mapping network from sampling space (z) to disentangled latent space (w) Source code in stylegan2_torch/generator/mapping.py 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , latent_dim : int , n_mlp : int , lr_mlp_mult : float ): super () . __init__ ( Normalize (), * [ EqualLeakyReLU ( latent_dim , latent_dim , lr_mult = lr_mlp_mult , ) for _ in range ( n_mlp ) ] )","title":"MappingNetwork"},{"location":"generator/#stylegan2_torch.generator.mapping.Normalize","text":"Bases: nn . Module Normalize latent vector for each sample","title":"Normalize"},{"location":"generator/#stylegan2_torch.generator.mapping.Normalize.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/generator/mapping.py 12 13 14 15 def forward ( self , input : Tensor ) -> Tensor : # input: (N, style_dim) # Normalize z in each sample to N(0,1) return input * torch . rsqrt ( torch . mean ( input ** 2 , dim = 1 , keepdim = True ) + 1e-8 )","title":"forward()"},{"location":"generator/#stylegan2_torch.generator.rgb","text":"","title":"rgb"},{"location":"generator/#stylegan2_torch.generator.rgb.ToRGB","text":"ToRGB ( in_channel : int , latent_dim : int , up : int , blur_kernel : List [ int ], ) Bases: nn . Module Source code in stylegan2_torch/generator/rgb.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , in_channel : int , latent_dim : int , up : int , blur_kernel : List [ int ], ): super () . __init__ () # Affine mapping from W to style vector self . affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 ) # Trainable parameters self . weight = Parameter ( torch . randn ( 1 , 1 , in_channel , 1 , 1 )) self . scale = 1 / math . sqrt ( in_channel ) self . bias = Parameter ( torch . zeros ( 1 , 1 , 1 , 1 )) if up > 1 : self . upsample = Upsample ( blur_kernel , up )","title":"ToRGB"},{"location":"generator/#stylegan2_torch.generator.rgb.ToRGB.affine","text":"affine = EqualLinear ( latent_dim , in_channel , bias_init = 1 )","title":"affine"},{"location":"generator/#stylegan2_torch.generator.rgb.ToRGB.bias","text":"bias = Parameter ( torch . zeros ( 1 , 1 , 1 , 1 ))","title":"bias"},{"location":"generator/#stylegan2_torch.generator.rgb.ToRGB.scale","text":"scale = 1 / math . sqrt ( in_channel )","title":"scale"},{"location":"generator/#stylegan2_torch.generator.rgb.ToRGB.upsample","text":"upsample = Upsample ( blur_kernel , up )","title":"upsample"},{"location":"generator/#stylegan2_torch.generator.rgb.ToRGB.weight","text":"weight = Parameter ( torch . randn ( 1 , 1 , in_channel , 1 , 1 ))","title":"weight"},{"location":"generator/#stylegan2_torch.generator.rgb.ToRGB.forward","text":"forward ( input : Tensor , w : Tensor , prev_output : Optional [ Tensor ] = None , ) -> Tensor Source code in stylegan2_torch/generator/rgb.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def forward ( self , input : Tensor , w : Tensor , prev_output : Optional [ Tensor ] = None ) -> Tensor : batch , in_channel , _ , _ = input . shape # Get style vectors (N, 1, C_in, 1, 1) style = self . affine ( w ) . view ( batch , 1 , in_channel , 1 , 1 ) # Modulate weights with equalized learning rate (N, C_out, C_in, K_h, K_w) weight = mod ( self . scale * self . weight , style ) # Perform convolution and add bias out = group_conv ( input , weight ) + self . bias if prev_output is not None : out = out + self . upsample ( prev_output ) return out","title":"forward()"},{"location":"generator/#stylegan2_torch.generator.rgb.Upsample","text":"Upsample ( blur_kernel : List [ int ], factor : int ) Bases: nn . Module Upsampling + apply blurring FIR filter Source code in stylegan2_torch/generator/rgb.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , blur_kernel : List [ int ], factor : int ): super () . __init__ () self . factor = factor # Factor to compensate for averaging with zeros self . kernel : Tensor self . register_buffer ( \"kernel\" , make_kernel ( blur_kernel , self . factor )) # Since upsampling by factor means there is factor - 1 pad1 already built-in \"\"\" UPSAMPLE CASE kernel: [kkkkk]................[kkkkk] (k_w = 5) upsampled: [x---x---x---x---x---x---] (in_w = 6, up_x = 4) padded: [ppppx---x---x---x---x---x---] (pad0 = 4, pad1 = 0) output: [oooooooooooooooooooooooo] (out_w = 24) Hence, pad0 + pad1 = k_w - 1 pad0 - pad1 = up_x - 1 DOWNSAMPLE CASE kernel: [kkkkk]...............[kkkkk] (k_w = 5) input: [xxxxxxxxxxxxxxxxxxxxxxxx] (in_w = 24) padded: [ppxxxxxxxxxxxxxxxxxxxxxxxxp] (pad0 = 2, pad1 = 1) output: [o-o-o-o-o-o-o-o-o-o-o-o] (out_w = 12) Since last (factor - 1) elements are discarded anyway, they don't need to be padded Hence, pad0 + pad1 = k_w - 1 - (factor - 1) pad0 - pad1 = 0 or 1 \"\"\" p = len ( blur_kernel ) - factor pad0 = ( p + 1 ) // 2 + factor - 1 pad1 = p // 2 self . pad = ( pad0 , pad1 )","title":"Upsample"},{"location":"generator/#stylegan2_torch.generator.rgb.Upsample.factor","text":"factor = factor","title":"factor"},{"location":"generator/#stylegan2_torch.generator.rgb.Upsample.kernel","text":"kernel : Tensor = None","title":"kernel"},{"location":"generator/#stylegan2_torch.generator.rgb.Upsample.pad","text":"pad = ( pad0 , pad1 )","title":"pad"},{"location":"generator/#stylegan2_torch.generator.rgb.Upsample.forward","text":"forward ( input : Tensor ) -> Tensor Source code in stylegan2_torch/generator/rgb.py 57 58 59 60 61 62 def forward ( self , input : Tensor ) -> Tensor : return upfirdn2d ( input , self . kernel , up = self . factor , down = 1 , pad = self . pad )","title":"forward()"},{"location":"loss/","text":"d_loss d_loss ( real_pred : Tensor , fake_pred : Tensor ) -> Tensor Calculates the discriminator loss. (equivalent to adversarial loss in original GAN paper). loss = softplus(-f(x)) + softplus(f(x)) Parameters: Name Type Description Default real_pred Tensor Predicted scores for real images required fake_pred Tensor Predicted scores for fake images required Returns: Name Type Description Tensor Tensor Discriminator loss Source code in stylegan2_torch/loss.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def d_loss ( real_pred : Tensor , fake_pred : Tensor , ) -> Tensor : \"\"\" Calculates the discriminator loss. (equivalent to adversarial loss in original GAN paper). loss = softplus(-f(x)) + softplus(f(x)) Args: real_pred (Tensor): Predicted scores for real images fake_pred (Tensor): Predicted scores for fake images Returns: Tensor: Discriminator loss \"\"\" real_loss = F . softplus ( - real_pred ) fake_loss = F . softplus ( fake_pred ) return real_loss . mean () + fake_loss . mean () d_reg_loss d_reg_loss ( real_pred : Tensor , real_img : Tensor ) -> Tensor Note The loss function was first proposed in https://arxiv.org/pdf/1801.04406.pdf . This regularization term penalizes the discriminator from producing a gradient orthogonal to the true data manifold (i.e. Expected gradient w.r.t. real image distribution should be zero). This means that: Discriminator score cannot improve once generator reaches true data distribution (because discriminator gives same expected score if inputs are from sample distribution, based on this regularization term) Near Nash equilibrium, discriminator is encouraged to minimize the gradient magnitude (because adversarial loss cannot improve, see 1) Points 1 and 2 are sort of chicken-and-egg in nature but the idea is to help converge to the Nash equilibrium. Calculates the discriminator R_1 loss. Source code in stylegan2_torch/loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def d_reg_loss ( real_pred : Tensor , real_img : Tensor ) -> Tensor : \"\"\" Calculates the discriminator R_1 loss. Note: The loss function was first proposed in [https://arxiv.org/pdf/1801.04406.pdf](https://arxiv.org/pdf/1801.04406.pdf). This regularization term penalizes the discriminator from producing a gradient orthogonal to the true data manifold (i.e. Expected gradient w.r.t. real image distribution should be zero). This means that: 1. Discriminator score cannot improve once generator reaches true data distribution (because discriminator gives same expected score if inputs are from sample distribution, based on this regularization term) 2. Near Nash equilibrium, discriminator is encouraged to minimize the gradient magnitude (because adversarial loss cannot improve, see 1) Points 1 and 2 are sort of chicken-and-egg in nature but the idea is to help converge to the Nash equilibrium. \"\"\" # Gradients w.r.t. convolution weights are not needed since only gradients w.r.t. input images are propagated with no_weight_grad (): # create_graph = true because we still need to use this gradient to perform backpropagation # real_pred.sum() is needed to obtain a scalar, but does not affect gradients (since each sample independently contributes to output) ( grad_real ,) = autograd . grad ( outputs = real_pred . sum (), inputs = real_img , create_graph = True ) grad_penalty = grad_real . pow ( 2 ) . reshape ( grad_real . shape [ 0 ], - 1 ) . sum ( 1 ) . mean () return grad_penalty g_loss g_loss ( fake_pred : Tensor ) -> Tensor Calculates the generator loss. Parameters: Name Type Description Default fake_pred Tensor Predicted scores for fake images required Returns: Name Type Description Tensor Tensor Generator loss Source code in stylegan2_torch/loss.py 62 63 64 65 66 67 68 69 70 71 72 73 74 def g_loss ( fake_pred : Tensor ) -> Tensor : \"\"\" Calculates the generator loss. Args: fake_pred (Tensor): Predicted scores for fake images Returns: Tensor: Generator loss \"\"\" loss = F . softplus ( - fake_pred ) . mean () return loss g_reg_loss g_reg_loss ( fake_img : Tensor , latents : Tensor , mean_path_length : Union [ Tensor , Literal [ 0 ]], decay : float = 0.01 , ) -> Tuple [ Tensor , Tensor , Tensor ] Calculates Generator path length regularization loss. Parameters: Name Type Description Default fake_img Tensor Generated images (N, C, H, W) required latents Tensor W+ latent vectors (N, P, 512), P = number of style vectors required mean_path_length Union [ Tensor , Literal [0]] Current accumulated mean path length (dynamic a ) required decay float Decay in accumulating a . Defaults to 0.01. 0.01 Returns: Type Description Tuple [ Tensor , Tensor , Tensor ] Tuple[Tensor, Tensor, Tensor]: Path loss, mean path, path length Note This loss function was first introduced in StyleGAN2. The idea is that fixed-sized steps in W results in fixed-magnitude change in image. Key Intuition : minimizing \\(\\mathbb{E}_{\\mathbf{w},\\mathbf{y}~N(0,1)}(||\\mathbf{J^T_{\\mathbf{w}}\\mathbf{y}}||_2 - a)^2\\) is equivalent to scaling \\(W+\\) equally in each dimension. Reason: Do SVD on \\(\\mathbf{J^T_{\\mathbf{w}}} = U \\bar{\\Sigma} V^T\\) \\(U\\) and \\(V\\) are orthogonal and hence irrelevant (since orthogonal matrices simply rotates the vector, but \\(\\mathbf{y}\\) is N(0,1), it is still the same distribution after rotation) \\(\\bar{\\Sigma}\\) has \\(L\\) non-zero singular values representing scaling factor in \\(L\\) dimensions Loss is minimized when \\(\\bar{\\Sigma}\\) has identical singular values equal \\(\\frac{a}{\\sqrt{L}}\\) (because high-dimensional normal distributions have norm centered around \\(\\sqrt{L}\\) ) Info Implementation: \\(a\\) is set dynamically using the moving average of the path_lengths (sort of like searching for the appropriate scaling factor in an non-agressive manner). As explained in paper's Appendix B, ideal weight for path regularization is \\(\\gamma_{pl} = \\frac{\\ln 2}{r^2(\\ln r - \\ln 2)}\\) . This is achieved by setting pl_weight , then in the code, the loss is first scaled by \\(r^2\\) (i.e. height * width) in noise then by n_layers in path_lengths by taken mean over the n_layers style vectors. Resulting is equivalent as saying that idea pl_weight is 2. See here . path_batch_shrink controls the fraction of batch size to use to reduce memory footprint of regularization. Since it is done without freeing the memory of the existing batch. Identity \\(\\mathbf{J^T_{\\mathbf{w}}} \\mathbf{y} = \\nabla (g(\\mathbf{w}) \\mathbf{y})\\) Source code in stylegan2_torch/loss.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def g_reg_loss ( fake_img : Tensor , latents : Tensor , mean_path_length : Union [ Tensor , Literal [ 0 ]], decay : float = 0.01 , ) -> Tuple [ Tensor , Tensor , Tensor ]: \"\"\" Calculates Generator path length regularization loss. Args: fake_img (Tensor): Generated images (N, C, H, W) latents (Tensor): W+ latent vectors (N, P, 512), P = number of style vectors mean_path_length (Union[Tensor, Literal[0]]): Current accumulated mean path length (dynamic `a`) decay (float, optional): Decay in accumulating `a`. Defaults to 0.01. Returns: Tuple[Tensor, Tensor, Tensor]: Path loss, mean path, path length Note: This loss function was first introduced in StyleGAN2. The idea is that fixed-sized steps in W results in fixed-magnitude change in image. **Key Intuition**: minimizing $\\mathbb{E}_{\\mathbf{w},\\mathbf{y}~N(0,1)}(||\\mathbf{J^T_{\\mathbf{w}}\\mathbf{y}}||_2 - a)^2$ is equivalent to scaling $W+$ equally in each dimension. Reason: 1. Do SVD on $\\mathbf{J^T_{\\mathbf{w}}} = U \\\\bar{\\Sigma} V^T$ 2. $U$ and $V$ are orthogonal and hence irrelevant (since orthogonal matrices simply rotates the vector, but $\\mathbf{y}$ is N(0,1), it is still the same distribution after rotation) 3. $\\\\bar{\\Sigma}$ has $L$ non-zero singular values representing scaling factor in $L$ dimensions 4. Loss is minimized when $\\\\bar{\\Sigma}$ has identical singular values equal $\\\\frac{a}{\\sqrt{L}}$ (because high-dimensional normal distributions have norm centered around $\\sqrt{L}$) Info: Implementation: 1. $a$ is set dynamically using the moving average of the path_lengths (sort of like searching for the appropriate scaling factor in an non-agressive manner). 2. As explained in paper's Appendix B, ideal weight for path regularization is $\\gamma_{pl} = \\\\frac{\\ln 2}{r^2(\\ln r - \\ln 2)}$. This is achieved by setting `pl_weight`, then in the code, the loss is first scaled by $r^2$ (i.e. height * width) in `noise` then by `n_layers` in `path_lengths` by taken mean over the `n_layers` style vectors. Resulting is equivalent as saying that idea `pl_weight` is 2. See [here](https://github.com/NVlabs/stylegan2/blob/master/training/loss.py). 3. `path_batch_shrink` controls the fraction of batch size to use to reduce memory footprint of regularization. Since it is done without freeing the memory of the existing batch. 4. Identity $\\mathbf{J^T_{\\mathbf{w}}} \\mathbf{y} = \\\\nabla (g(\\mathbf{w}) \\mathbf{y})$ \"\"\" noise = torch . randn_like ( fake_img ) / math . sqrt ( fake_img . shape [ 2 ] * fake_img . shape [ 3 ] ) ( grad ,) = autograd . grad ( outputs = ( fake_img * noise ) . sum (), inputs = latents , create_graph = True ) path_lengths = torch . sqrt ( grad . pow ( 2 ) . sum ( 2 ) . mean ( 1 )) path_mean = mean_path_length + decay * ( path_lengths . mean () - mean_path_length ) path_penalty = ( path_lengths - path_mean ) . pow ( 2 ) . mean () return path_penalty , path_mean . detach (), path_lengths","title":"loss"},{"location":"loss/#stylegan2_torch.loss.d_loss","text":"d_loss ( real_pred : Tensor , fake_pred : Tensor ) -> Tensor Calculates the discriminator loss. (equivalent to adversarial loss in original GAN paper). loss = softplus(-f(x)) + softplus(f(x)) Parameters: Name Type Description Default real_pred Tensor Predicted scores for real images required fake_pred Tensor Predicted scores for fake images required Returns: Name Type Description Tensor Tensor Discriminator loss Source code in stylegan2_torch/loss.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def d_loss ( real_pred : Tensor , fake_pred : Tensor , ) -> Tensor : \"\"\" Calculates the discriminator loss. (equivalent to adversarial loss in original GAN paper). loss = softplus(-f(x)) + softplus(f(x)) Args: real_pred (Tensor): Predicted scores for real images fake_pred (Tensor): Predicted scores for fake images Returns: Tensor: Discriminator loss \"\"\" real_loss = F . softplus ( - real_pred ) fake_loss = F . softplus ( fake_pred ) return real_loss . mean () + fake_loss . mean ()","title":"d_loss()"},{"location":"loss/#stylegan2_torch.loss.d_reg_loss","text":"d_reg_loss ( real_pred : Tensor , real_img : Tensor ) -> Tensor Note The loss function was first proposed in https://arxiv.org/pdf/1801.04406.pdf . This regularization term penalizes the discriminator from producing a gradient orthogonal to the true data manifold (i.e. Expected gradient w.r.t. real image distribution should be zero). This means that: Discriminator score cannot improve once generator reaches true data distribution (because discriminator gives same expected score if inputs are from sample distribution, based on this regularization term) Near Nash equilibrium, discriminator is encouraged to minimize the gradient magnitude (because adversarial loss cannot improve, see 1) Points 1 and 2 are sort of chicken-and-egg in nature but the idea is to help converge to the Nash equilibrium. Calculates the discriminator R_1 loss. Source code in stylegan2_torch/loss.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def d_reg_loss ( real_pred : Tensor , real_img : Tensor ) -> Tensor : \"\"\" Calculates the discriminator R_1 loss. Note: The loss function was first proposed in [https://arxiv.org/pdf/1801.04406.pdf](https://arxiv.org/pdf/1801.04406.pdf). This regularization term penalizes the discriminator from producing a gradient orthogonal to the true data manifold (i.e. Expected gradient w.r.t. real image distribution should be zero). This means that: 1. Discriminator score cannot improve once generator reaches true data distribution (because discriminator gives same expected score if inputs are from sample distribution, based on this regularization term) 2. Near Nash equilibrium, discriminator is encouraged to minimize the gradient magnitude (because adversarial loss cannot improve, see 1) Points 1 and 2 are sort of chicken-and-egg in nature but the idea is to help converge to the Nash equilibrium. \"\"\" # Gradients w.r.t. convolution weights are not needed since only gradients w.r.t. input images are propagated with no_weight_grad (): # create_graph = true because we still need to use this gradient to perform backpropagation # real_pred.sum() is needed to obtain a scalar, but does not affect gradients (since each sample independently contributes to output) ( grad_real ,) = autograd . grad ( outputs = real_pred . sum (), inputs = real_img , create_graph = True ) grad_penalty = grad_real . pow ( 2 ) . reshape ( grad_real . shape [ 0 ], - 1 ) . sum ( 1 ) . mean () return grad_penalty","title":"d_reg_loss()"},{"location":"loss/#stylegan2_torch.loss.g_loss","text":"g_loss ( fake_pred : Tensor ) -> Tensor Calculates the generator loss. Parameters: Name Type Description Default fake_pred Tensor Predicted scores for fake images required Returns: Name Type Description Tensor Tensor Generator loss Source code in stylegan2_torch/loss.py 62 63 64 65 66 67 68 69 70 71 72 73 74 def g_loss ( fake_pred : Tensor ) -> Tensor : \"\"\" Calculates the generator loss. Args: fake_pred (Tensor): Predicted scores for fake images Returns: Tensor: Generator loss \"\"\" loss = F . softplus ( - fake_pred ) . mean () return loss","title":"g_loss()"},{"location":"loss/#stylegan2_torch.loss.g_reg_loss","text":"g_reg_loss ( fake_img : Tensor , latents : Tensor , mean_path_length : Union [ Tensor , Literal [ 0 ]], decay : float = 0.01 , ) -> Tuple [ Tensor , Tensor , Tensor ] Calculates Generator path length regularization loss. Parameters: Name Type Description Default fake_img Tensor Generated images (N, C, H, W) required latents Tensor W+ latent vectors (N, P, 512), P = number of style vectors required mean_path_length Union [ Tensor , Literal [0]] Current accumulated mean path length (dynamic a ) required decay float Decay in accumulating a . Defaults to 0.01. 0.01 Returns: Type Description Tuple [ Tensor , Tensor , Tensor ] Tuple[Tensor, Tensor, Tensor]: Path loss, mean path, path length Note This loss function was first introduced in StyleGAN2. The idea is that fixed-sized steps in W results in fixed-magnitude change in image. Key Intuition : minimizing \\(\\mathbb{E}_{\\mathbf{w},\\mathbf{y}~N(0,1)}(||\\mathbf{J^T_{\\mathbf{w}}\\mathbf{y}}||_2 - a)^2\\) is equivalent to scaling \\(W+\\) equally in each dimension. Reason: Do SVD on \\(\\mathbf{J^T_{\\mathbf{w}}} = U \\bar{\\Sigma} V^T\\) \\(U\\) and \\(V\\) are orthogonal and hence irrelevant (since orthogonal matrices simply rotates the vector, but \\(\\mathbf{y}\\) is N(0,1), it is still the same distribution after rotation) \\(\\bar{\\Sigma}\\) has \\(L\\) non-zero singular values representing scaling factor in \\(L\\) dimensions Loss is minimized when \\(\\bar{\\Sigma}\\) has identical singular values equal \\(\\frac{a}{\\sqrt{L}}\\) (because high-dimensional normal distributions have norm centered around \\(\\sqrt{L}\\) ) Info Implementation: \\(a\\) is set dynamically using the moving average of the path_lengths (sort of like searching for the appropriate scaling factor in an non-agressive manner). As explained in paper's Appendix B, ideal weight for path regularization is \\(\\gamma_{pl} = \\frac{\\ln 2}{r^2(\\ln r - \\ln 2)}\\) . This is achieved by setting pl_weight , then in the code, the loss is first scaled by \\(r^2\\) (i.e. height * width) in noise then by n_layers in path_lengths by taken mean over the n_layers style vectors. Resulting is equivalent as saying that idea pl_weight is 2. See here . path_batch_shrink controls the fraction of batch size to use to reduce memory footprint of regularization. Since it is done without freeing the memory of the existing batch. Identity \\(\\mathbf{J^T_{\\mathbf{w}}} \\mathbf{y} = \\nabla (g(\\mathbf{w}) \\mathbf{y})\\) Source code in stylegan2_torch/loss.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def g_reg_loss ( fake_img : Tensor , latents : Tensor , mean_path_length : Union [ Tensor , Literal [ 0 ]], decay : float = 0.01 , ) -> Tuple [ Tensor , Tensor , Tensor ]: \"\"\" Calculates Generator path length regularization loss. Args: fake_img (Tensor): Generated images (N, C, H, W) latents (Tensor): W+ latent vectors (N, P, 512), P = number of style vectors mean_path_length (Union[Tensor, Literal[0]]): Current accumulated mean path length (dynamic `a`) decay (float, optional): Decay in accumulating `a`. Defaults to 0.01. Returns: Tuple[Tensor, Tensor, Tensor]: Path loss, mean path, path length Note: This loss function was first introduced in StyleGAN2. The idea is that fixed-sized steps in W results in fixed-magnitude change in image. **Key Intuition**: minimizing $\\mathbb{E}_{\\mathbf{w},\\mathbf{y}~N(0,1)}(||\\mathbf{J^T_{\\mathbf{w}}\\mathbf{y}}||_2 - a)^2$ is equivalent to scaling $W+$ equally in each dimension. Reason: 1. Do SVD on $\\mathbf{J^T_{\\mathbf{w}}} = U \\\\bar{\\Sigma} V^T$ 2. $U$ and $V$ are orthogonal and hence irrelevant (since orthogonal matrices simply rotates the vector, but $\\mathbf{y}$ is N(0,1), it is still the same distribution after rotation) 3. $\\\\bar{\\Sigma}$ has $L$ non-zero singular values representing scaling factor in $L$ dimensions 4. Loss is minimized when $\\\\bar{\\Sigma}$ has identical singular values equal $\\\\frac{a}{\\sqrt{L}}$ (because high-dimensional normal distributions have norm centered around $\\sqrt{L}$) Info: Implementation: 1. $a$ is set dynamically using the moving average of the path_lengths (sort of like searching for the appropriate scaling factor in an non-agressive manner). 2. As explained in paper's Appendix B, ideal weight for path regularization is $\\gamma_{pl} = \\\\frac{\\ln 2}{r^2(\\ln r - \\ln 2)}$. This is achieved by setting `pl_weight`, then in the code, the loss is first scaled by $r^2$ (i.e. height * width) in `noise` then by `n_layers` in `path_lengths` by taken mean over the `n_layers` style vectors. Resulting is equivalent as saying that idea `pl_weight` is 2. See [here](https://github.com/NVlabs/stylegan2/blob/master/training/loss.py). 3. `path_batch_shrink` controls the fraction of batch size to use to reduce memory footprint of regularization. Since it is done without freeing the memory of the existing batch. 4. Identity $\\mathbf{J^T_{\\mathbf{w}}} \\mathbf{y} = \\\\nabla (g(\\mathbf{w}) \\mathbf{y})$ \"\"\" noise = torch . randn_like ( fake_img ) / math . sqrt ( fake_img . shape [ 2 ] * fake_img . shape [ 3 ] ) ( grad ,) = autograd . grad ( outputs = ( fake_img * noise ) . sum (), inputs = latents , create_graph = True ) path_lengths = torch . sqrt ( grad . pow ( 2 ) . sum ( 2 ) . mean ( 1 )) path_mean = mean_path_length + decay * ( path_lengths . mean () - mean_path_length ) path_penalty = ( path_lengths - path_mean ) . pow ( 2 ) . mean () return path_penalty , path_mean . detach (), path_lengths","title":"g_reg_loss()"},{"location":"utils/","text":"C module-attribute C = TypeVar ( 'C' , bound = Callable ) Resolution module-attribute Resolution = Literal [ 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 , 8192 ] T module-attribute T = TypeVar ( 'T' ) default_channels module-attribute default_channels : Dict [ Resolution , int ] = { 4 : 512 , 8 : 512 , 16 : 512 , 32 : 512 , 64 : 512 , 128 : 256 , 256 : 128 , 512 : 64 , 1024 : 32 , } accumulate accumulate ( model1 : nn . Module , model2 : nn . Module , decay : float = 0.5 ** ( 32 / ( 10 * 1000 )), ) -> None Accumulate parameters of model2 onto model1 using EMA Source code in stylegan2_torch/utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 def accumulate ( model1 : nn . Module , model2 : nn . Module , decay : float = 0.5 ** ( 32 / ( 10 * 1000 )), ) -> None : \"\"\" Accumulate parameters of model2 onto model1 using EMA \"\"\" par1 = dict ( model1 . named_parameters ()) par2 = dict ( model2 . named_parameters ()) for k in par1 . keys (): par1 [ k ] . data . mul_ ( decay ) . add_ ( par2 [ k ] . data , alpha = 1 - decay ) make_kernel make_kernel ( k : List [ int ], factor : int = 1 ) -> Tensor Creates 2D kernel from 1D kernel, compensating for zero-padded upsampling factor Source code in stylegan2_torch/utils.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def make_kernel ( k : List [ int ], factor : int = 1 , ) -> Tensor : \"\"\" Creates 2D kernel from 1D kernel, compensating for zero-padded upsampling factor \"\"\" kernel = torch . tensor ( k , dtype = torch . float32 ) kernel = kernel [ None , :] * kernel [:, None ] kernel /= kernel . sum () kernel *= factor ** 2 return kernel make_noise make_noise ( batch : int , latent_dim : int , n_noise : int , device : str ) Makes a random, normally distributed latent vector. Source code in stylegan2_torch/utils.py 58 59 60 61 62 63 def make_noise ( batch : int , latent_dim : int , n_noise : int , device : str ): \"\"\" Makes a random, normally distributed latent vector. \"\"\" return torch . randn ( n_noise , batch , latent_dim , device = device ) . unbind ( 0 ) mixing_noise mixing_noise ( batch : int , latent_dim : int , prob : float , device : str ) Makes a random, normally distributed latent vector. Returns a pair if mixing regularization. Source code in stylegan2_torch/utils.py 66 67 68 69 70 71 72 73 74 def mixing_noise ( batch : int , latent_dim : int , prob : float , device : str ): \"\"\" Makes a random, normally distributed latent vector. Returns a pair if mixing regularization. \"\"\" if random . random () < prob : return make_noise ( batch , latent_dim , 2 , device ) else : return [ make_noise ( batch , latent_dim , 1 , device )] proxy proxy ( f : C ) -> C Proxy function signature map for Module.__call__ type hint. Source code in stylegan2_torch/utils.py 77 78 79 80 81 def proxy ( f : C ) -> C : \"\"\" Proxy function signature map for `Module.__call__` type hint. \"\"\" return cast ( C , lambda self , * x , ** y : super ( self . __class__ , self ) . __call__ ( * x , ** y ))","title":"utils"},{"location":"utils/#stylegan2_torch.utils.C","text":"C = TypeVar ( 'C' , bound = Callable )","title":"C"},{"location":"utils/#stylegan2_torch.utils.Resolution","text":"Resolution = Literal [ 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 , 4096 , 8192 ]","title":"Resolution"},{"location":"utils/#stylegan2_torch.utils.T","text":"T = TypeVar ( 'T' )","title":"T"},{"location":"utils/#stylegan2_torch.utils.default_channels","text":"default_channels : Dict [ Resolution , int ] = { 4 : 512 , 8 : 512 , 16 : 512 , 32 : 512 , 64 : 512 , 128 : 256 , 256 : 128 , 512 : 64 , 1024 : 32 , }","title":"default_channels"},{"location":"utils/#stylegan2_torch.utils.accumulate","text":"accumulate ( model1 : nn . Module , model2 : nn . Module , decay : float = 0.5 ** ( 32 / ( 10 * 1000 )), ) -> None Accumulate parameters of model2 onto model1 using EMA Source code in stylegan2_torch/utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 def accumulate ( model1 : nn . Module , model2 : nn . Module , decay : float = 0.5 ** ( 32 / ( 10 * 1000 )), ) -> None : \"\"\" Accumulate parameters of model2 onto model1 using EMA \"\"\" par1 = dict ( model1 . named_parameters ()) par2 = dict ( model2 . named_parameters ()) for k in par1 . keys (): par1 [ k ] . data . mul_ ( decay ) . add_ ( par2 [ k ] . data , alpha = 1 - decay )","title":"accumulate()"},{"location":"utils/#stylegan2_torch.utils.make_kernel","text":"make_kernel ( k : List [ int ], factor : int = 1 ) -> Tensor Creates 2D kernel from 1D kernel, compensating for zero-padded upsampling factor Source code in stylegan2_torch/utils.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def make_kernel ( k : List [ int ], factor : int = 1 , ) -> Tensor : \"\"\" Creates 2D kernel from 1D kernel, compensating for zero-padded upsampling factor \"\"\" kernel = torch . tensor ( k , dtype = torch . float32 ) kernel = kernel [ None , :] * kernel [:, None ] kernel /= kernel . sum () kernel *= factor ** 2 return kernel","title":"make_kernel()"},{"location":"utils/#stylegan2_torch.utils.make_noise","text":"make_noise ( batch : int , latent_dim : int , n_noise : int , device : str ) Makes a random, normally distributed latent vector. Source code in stylegan2_torch/utils.py 58 59 60 61 62 63 def make_noise ( batch : int , latent_dim : int , n_noise : int , device : str ): \"\"\" Makes a random, normally distributed latent vector. \"\"\" return torch . randn ( n_noise , batch , latent_dim , device = device ) . unbind ( 0 )","title":"make_noise()"},{"location":"utils/#stylegan2_torch.utils.mixing_noise","text":"mixing_noise ( batch : int , latent_dim : int , prob : float , device : str ) Makes a random, normally distributed latent vector. Returns a pair if mixing regularization. Source code in stylegan2_torch/utils.py 66 67 68 69 70 71 72 73 74 def mixing_noise ( batch : int , latent_dim : int , prob : float , device : str ): \"\"\" Makes a random, normally distributed latent vector. Returns a pair if mixing regularization. \"\"\" if random . random () < prob : return make_noise ( batch , latent_dim , 2 , device ) else : return [ make_noise ( batch , latent_dim , 1 , device )]","title":"mixing_noise()"},{"location":"utils/#stylegan2_torch.utils.proxy","text":"proxy ( f : C ) -> C Proxy function signature map for Module.__call__ type hint. Source code in stylegan2_torch/utils.py 77 78 79 80 81 def proxy ( f : C ) -> C : \"\"\" Proxy function signature map for `Module.__call__` type hint. \"\"\" return cast ( C , lambda self , * x , ** y : super ( self . __class__ , self ) . __call__ ( * x , ** y ))","title":"proxy()"}]}